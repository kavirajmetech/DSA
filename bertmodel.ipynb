{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9448963,"sourceType":"datasetVersion","datasetId":5743086},{"sourceId":9941909,"sourceType":"datasetVersion","datasetId":6112794},{"sourceId":118881,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":99966,"modelId":124137},{"sourceId":119356,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":100370,"modelId":124137},{"sourceId":120158,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":101071,"modelId":125250}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport torch\nimport re\nimport random\nimport torch\nfrom torch.utils.data import Dataset\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordPiece\nfrom tokenizers.pre_tokenizers import Whitespace\nfrom tokenizers.trainers import WordPieceTrainer\nimport random\nfrom tokenizers import BertWordPieceTokenizer\nfrom transformers import BertTokenizer\nimport tqdm\nfrom torch.utils.data import Dataset, DataLoader\nimport itertools\nimport math\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch.optim import Adam","metadata":{"id":"Z_p8twHVZDLx","execution":{"iopub.status.busy":"2024-11-19T05:04:42.752606Z","iopub.execute_input":"2024-11-19T05:04:42.753509Z","iopub.status.idle":"2024-11-19T05:04:42.812981Z","shell.execute_reply.started":"2024-11-19T05:04:42.753469Z","shell.execute_reply":"2024-11-19T05:04:42.812332Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"merge= r'/kaggle/input/wikidata/diamond_merged.txt' # Parimelagar urai with its explanation and  Random Merge 20 times\n# train = r'../datasets/main_train_train.txt'\ntest = r'/kaggle/input/wikidata/main_train_valid.txt' # total 400MB\niconst=r'/kaggle/input/iconst/data.txt'","metadata":{"id":"_-7ywb1aZcNn","execution":{"iopub.status.busy":"2024-11-19T05:04:43.377873Z","iopub.execute_input":"2024-11-19T05:04:43.378158Z","iopub.status.idle":"2024-11-19T05:04:43.382614Z","shell.execute_reply.started":"2024-11-19T05:04:43.378131Z","shell.execute_reply":"2024-11-19T05:04:43.381622Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"conv=[]\n# with open( merge, 'r',encoding='utf-8') as c:\n#     merged = c.readlines()           \n# # with open( train, 'r',encoding='utf-8') as c:\n# #     trained = c.readlines()\n# with open( test, 'r',encoding='utf-8') as c:\n#     tested = c.readlines()\nwith open( iconst, 'r',encoding='utf-8') as c:\n    icst = c.readlines()  \nconv=icst","metadata":{"id":"qcAna19JZtnb","execution":{"iopub.status.busy":"2024-11-19T05:04:43.819783Z","iopub.execute_input":"2024-11-19T05:04:43.820092Z","iopub.status.idle":"2024-11-19T05:04:43.830664Z","shell.execute_reply.started":"2024-11-19T05:04:43.820064Z","shell.execute_reply":"2024-11-19T05:04:43.829809Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# conv=tested+merged\n","metadata":{"execution":{"iopub.status.busy":"2024-11-19T05:04:44.443059Z","iopub.execute_input":"2024-11-19T05:04:44.443395Z","iopub.status.idle":"2024-11-19T05:04:44.447486Z","shell.execute_reply.started":"2024-11-19T05:04:44.443365Z","shell.execute_reply":"2024-11-19T05:04:44.446505Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"len(conv)#,len(trained),len(tested) Total  sentences","metadata":{"execution":{"iopub.status.busy":"2024-11-19T05:04:45.193713Z","iopub.execute_input":"2024-11-19T05:04:45.194058Z","iopub.status.idle":"2024-11-19T05:04:45.200630Z","shell.execute_reply.started":"2024-11-19T05:04:45.194027Z","shell.execute_reply":"2024-11-19T05:04:45.199756Z"},"trusted":true},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"6396"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"sum=0\nl=0\ndic={}\nword_freq_count={}\ns=set()\nfor sent in conv:\n    length=len(sent.split())\n    l=max(l,length)\n    for word in sent.split():\n        if word in word_freq_count.keys():\n            word_freq_count[word]=word_freq_count[word]+1\n        else:\n            word_freq_count[word]=1\n        s.add(word)\n    dic[length]=1 if length not in dic.keys() else dic[length]+1\nlen(s),l                                                           ","metadata":{"execution":{"iopub.status.busy":"2024-11-19T05:04:45.725437Z","iopub.execute_input":"2024-11-19T05:04:45.725774Z","iopub.status.idle":"2024-11-19T05:04:45.825974Z","shell.execute_reply.started":"2024-11-19T05:04:45.725746Z","shell.execute_reply":"2024-11-19T05:04:45.825091Z"},"trusted":true},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(22324, 233)"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"word_freq_count_above_10={}\nfor key,value in word_freq_count.items():\n    if value>10:\n        word_freq_count_above_10[key]=value\nprint(len(word_freq_count_above_10))","metadata":{"execution":{"iopub.status.busy":"2024-11-19T05:04:46.215948Z","iopub.execute_input":"2024-11-19T05:04:46.216673Z","iopub.status.idle":"2024-11-19T05:04:46.223721Z","shell.execute_reply.started":"2024-11-19T05:04:46.216638Z","shell.execute_reply":"2024-11-19T05:04:46.222795Z"},"trusted":true},"outputs":[{"name":"stdout","text":"1769\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"sorted_dict = dict(sorted(dic.items()))\nsorted_dict    #sentence Length","metadata":{"execution":{"iopub.status.busy":"2024-11-19T05:04:46.681588Z","iopub.execute_input":"2024-11-19T05:04:46.681886Z","iopub.status.idle":"2024-11-19T05:04:46.689525Z","shell.execute_reply.started":"2024-11-19T05:04:46.681858Z","shell.execute_reply":"2024-11-19T05:04:46.688558Z"},"trusted":true},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"{0: 2,\n 1: 24,\n 2: 72,\n 3: 93,\n 4: 109,\n 5: 122,\n 6: 128,\n 7: 142,\n 8: 142,\n 9: 141,\n 10: 129,\n 11: 129,\n 12: 135,\n 13: 141,\n 14: 126,\n 15: 141,\n 16: 130,\n 17: 136,\n 18: 151,\n 19: 152,\n 20: 148,\n 21: 109,\n 22: 142,\n 23: 128,\n 24: 134,\n 25: 122,\n 26: 118,\n 27: 133,\n 28: 126,\n 29: 84,\n 30: 112,\n 31: 116,\n 32: 102,\n 33: 94,\n 34: 122,\n 35: 98,\n 36: 105,\n 37: 93,\n 38: 120,\n 39: 106,\n 40: 122,\n 41: 112,\n 42: 150,\n 43: 164,\n 44: 145,\n 45: 174,\n 46: 161,\n 47: 140,\n 48: 105,\n 49: 93,\n 50: 95,\n 51: 55,\n 52: 41,\n 53: 36,\n 54: 27,\n 55: 14,\n 56: 12,\n 57: 12,\n 58: 7,\n 59: 7,\n 60: 5,\n 61: 3,\n 62: 3,\n 63: 1,\n 65: 3,\n 66: 1,\n 67: 2,\n 69: 3,\n 70: 2,\n 71: 4,\n 72: 3,\n 73: 1,\n 75: 2,\n 77: 1,\n 78: 1,\n 80: 1,\n 81: 1,\n 82: 2,\n 83: 1,\n 85: 1,\n 87: 2,\n 90: 1,\n 92: 1,\n 94: 1,\n 96: 1,\n 97: 1,\n 99: 2,\n 104: 1,\n 105: 1,\n 107: 1,\n 109: 1,\n 114: 1,\n 116: 1,\n 118: 1,\n 122: 1,\n 131: 1,\n 132: 1,\n 151: 1,\n 156: 1,\n 159: 1,\n 161: 1,\n 165: 1,\n 180: 1,\n 233: 1}"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"process labels","metadata":{}},{"cell_type":"code","source":"import json\n\nwith open(r'/kaggle/input/iconst/labels.json', 'r') as f:\n    data_v = json.load(f)\n\nlabels = [data_v[key] for key in sorted(data_v.keys())]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T05:04:47.399583Z","iopub.execute_input":"2024-11-19T05:04:47.399947Z","iopub.status.idle":"2024-11-19T05:04:47.412888Z","shell.execute_reply.started":"2024-11-19T05:04:47.399914Z","shell.execute_reply":"2024-11-19T05:04:47.411832Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels[:10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T05:04:48.427957Z","iopub.execute_input":"2024-11-19T05:04:48.428656Z","iopub.status.idle":"2024-11-19T05:04:48.434563Z","shell.execute_reply.started":"2024-11-19T05:04:48.428618Z","shell.execute_reply":"2024-11-19T05:04:48.433572Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"[['0', '0', '0', '0'],\n ['1', '1', '2', '3'],\n ['1', '0', '1', '1'],\n ['1', '1', '2', '1'],\n ['1', '0', '1', '1'],\n ['0', '0', '0', '0'],\n ['0', '0', '0', '0'],\n ['1', '1', '2', '1'],\n ['0', '0', '0', '0'],\n ['1', '0', '2', '2']]"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# import string\n# def remove_punctuation(sentences):\n#     translator = str.maketrans('', '', string.punctuation)\n#     cleaned_sentences = []\n#     for sentence in sentences:\n#         words = sentence.translate(translator).split()\n#         cleaned_sentence = \" \".join(words)\n#         cleaned_sentences.append(cleaned_sentence)\n    \n#     return cleaned_sentences","metadata":{"execution":{"iopub.status.busy":"2024-11-19T05:04:49.160758Z","iopub.execute_input":"2024-11-19T05:04:49.161063Z","iopub.status.idle":"2024-11-19T05:04:49.165136Z","shell.execute_reply.started":"2024-11-19T05:04:49.161035Z","shell.execute_reply":"2024-11-19T05:04:49.164183Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# # # conv=trained\n# train_cleaned=remove_punctuation(trained)\n# test_cleaned=remove_punctuation(tested)","metadata":{"execution":{"iopub.status.busy":"2024-11-19T05:04:49.989950Z","iopub.execute_input":"2024-11-19T05:04:49.990282Z","iopub.status.idle":"2024-11-19T05:04:49.994350Z","shell.execute_reply.started":"2024-11-19T05:04:49.990253Z","shell.execute_reply":"2024-11-19T05:04:49.993268Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# with open('main_train_train.txt','a',encoding='utf-8') as f:\n#     for line in train_cleaned:\n#         if len(line.split())<=64: \n#             f.writelines(f'{line}\\n')\n# with open('main_train_valid.txt','a',encoding='utf-8') as f:\n#     for line in test_cleaned:\n#         if len(line.split())<=64: \n#             f.writelines(f'{line}\\n')\n    ","metadata":{"execution":{"iopub.status.busy":"2024-11-19T05:04:50.510443Z","iopub.execute_input":"2024-11-19T05:04:50.511181Z","iopub.status.idle":"2024-11-19T05:04:50.514876Z","shell.execute_reply.started":"2024-11-19T05:04:50.511148Z","shell.execute_reply":"2024-11-19T05:04:50.513938Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"conv[:5]","metadata":{"execution":{"iopub.status.busy":"2024-11-19T05:04:51.011947Z","iopub.execute_input":"2024-11-19T05:04:51.012260Z","iopub.status.idle":"2024-11-19T05:04:51.017985Z","shell.execute_reply.started":"2024-11-19T05:04:51.012230Z","shell.execute_reply":"2024-11-19T05:04:51.017050Z"},"trusted":true},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"['USER Abhi tak 2000 ke note me mujhe GPS nano chip nahin mila\\n',\n 'USER USER Abe katiye tumse kuch huaa toh jata nahi bas bakri jaise maymay karte rehte ho behan ke lodo tumhari Tablighi jamaat ke karan corona faila aur Bhoomi Poojan mein sab ko screen aur full medical jach ke saath huaa bhosdiwalo\\n',\n 'USER Ye sab sazish hai bina saman ke koi kaise apne gher ja sakta hai dekh lena inn logo ke beech bahut aise jamaat ke wo corona honge jo delhi se nikle they\\n',\n 'abe jao tum to dasko pahle hi fash gye the jab tere dada ne talwar ke nokh par salwar pahena tha tera daram kabhi hindu kabhi muslim\\n',\n 'Ab ye afbah kaun faila Raha hai ki Shahhen bag ke Biryani me Population control ki Dawa Milli Hui hai\\n']"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# conv=trained\n# print(len(conv),len(trained))","metadata":{"execution":{"iopub.status.busy":"2024-11-19T05:04:51.524701Z","iopub.execute_input":"2024-11-19T05:04:51.525602Z","iopub.status.idle":"2024-11-19T05:04:51.529148Z","shell.execute_reply.started":"2024-11-19T05:04:51.525566Z","shell.execute_reply":"2024-11-19T05:04:51.528388Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# paths=['/kaggle/input/wikidata/diamond_merged.txt','/kaggle/input/wikidata/main_train_valid.txt' ]\npaths=['/kaggle/input/iconst/data.txt']","metadata":{"id":"s-l8YYCfc3pp","execution":{"iopub.status.busy":"2024-11-19T05:04:52.344183Z","iopub.execute_input":"2024-11-19T05:04:52.344812Z","iopub.status.idle":"2024-11-19T05:04:52.348779Z","shell.execute_reply.started":"2024-11-19T05:04:52.344779Z","shell.execute_reply":"2024-11-19T05:04:52.347917Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom tokenizers import Tokenizer, models, pre_tokenizers, processors\nimport random\nimport math\nimport torch.nn.functional as F\nfrom torch.optim import Adam\nfrom tqdm import tqdm\nfrom tokenizers import trainers\nfrom tokenizers import Tokenizer, models, trainers, pre_tokenizers, processors\nimport torch\nfrom torch.utils.data import Dataset\nimport random\nimport json\nfrom torch.optim import AdamW\nfrom transformers import get_linear_schedule_with_warmup\n\nSEQ_LEN = 68\nEMBED_LEN = 252\nVOCAB_SIZE = 20000   #vocab size\n\ndef create_tokenizer(paths, vocab_size=VOCAB_SIZE):\n    tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n    tokenizer.pre_tokenizer = Whitespace()\n    special_tokens = [\"[CLS1]\",\"[CLS2]\",\"[PAD]\", \"[MASK]\", \"[UNK]\"]\n    tokenizer.add_special_tokens(special_tokens)\n\n    def batch_iterator(file_paths, batch_size=64):\n        for path in file_paths:\n            with open(path, 'r', encoding='utf-8') as f:\n                for line in f:\n                    yield line.strip()\n\n    trainer = WordPieceTrainer(\n        vocab_size=vocab_size,\n        special_tokens=special_tokens\n    )\n    tokenizer.train_from_iterator(batch_iterator(paths), trainer=trainer)\n    \n    return tokenizer\n\nclass BERTDataset(Dataset):\n    def __init__(self, sentences, tokenizer, labels,seq_len=SEQ_LEN):\n        self.tokenizer = tokenizer\n        self.seq_len = seq_len\n        self.sentences = sentences\n        self.labels=labels\n\n    def __len__(self):\n        return len(self.sentences)\n\n    def __getitem__(self, item,flag=False):\n        sentence = self.sentences[item]\n        encoding = self.tokenizer.encode(sentence)\n        \n        # List of special class tokens\n        classes = [\n            self.tokenizer.token_to_id('[CLS1]'),\n            self.tokenizer.token_to_id('[CLS2]'),\n            # self.tokenizer.token_to_id('[CLS3]'),\n            # self.tokenizer.token_to_id('[CLS4]')\n        ]\n        \n        tokens = classes + encoding.ids  \n        if len(tokens) > self.seq_len:\n            tokens = tokens[:self.seq_len]\n        \n        padding_length = self.seq_len - len(tokens)\n        tokens = tokens + [self.tokenizer.token_to_id('[PAD]')] * padding_length\n        \n        mask = [1 if token != self.tokenizer.token_to_id('[PAD]') else 0 for token in tokens]\n        \n        bert_input = tokens.copy()\n        bert_label = [int(self.labels[item][0]), int(self.labels[item][1]), int(self.labels[item][2]), int(self.labels[item][3])]\n    \n        if flag:\n            for i in range(len(tokens)):\n                if mask[i] == 1 and random.random() < 0.25:\n                    if random.random() < 0.8:\n                        bert_input[i] = self.tokenizer.token_to_id('[MASK]')\n                    elif random.random() < 0.5:\n                        bert_input[i] = random.randint(0, self.tokenizer.get_vocab_size() - 1)\n        \n        return {\n            'bert_input': torch.tensor(bert_input),\n            'bert_label': torch.tensor(bert_label),\n            'attention_mask': torch.tensor(mask)\n        }\n\n    \nclass PositionalEmbedding(torch.nn.Module):\n    def __init__(self, d_model, max_len=SEQ_LEN):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model).float()\n        print(pe.shape)\n        pe.require_grad = False\n        position = torch.arange(0, max_len).float().unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n        print('div_term',div_term)\n        print('div_term_dim',div_term.shape)\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term) \n        print('pe s',pe.shape)\n        print('pe',pe)\n        self.register_buffer('pe', pe.unsqueeze(0))\n\n    def forward(self, x):\n        return self.pe[:, :x.size(1)]\n\nclass BERTEmbedding(torch.nn.Module):\n    def __init__(self, vocab_size, embed_size, seq_len=SEQ_LEN, dropout=0.1):\n        super().__init__()\n        self.token = torch.nn.Embedding(vocab_size, embed_size, padding_idx=0)\n        self.position = PositionalEmbedding(embed_size, seq_len)\n        self.dropout = torch.nn.Dropout(p=dropout)\n        self.embed_size = embed_size\n\n    def forward(self, sequence):\n        x = self.token(sequence) + self.position(sequence)\n        return self.dropout(x)\n\nclass MultiHeadedAttention(torch.nn.Module):\n    def __init__(self, heads, d_model, dropout=0.1):\n        super(MultiHeadedAttention, self).__init__()\n        assert d_model % heads == 0\n        self.d_k = d_model // heads\n        self.heads = heads\n        self.dropout = torch.nn.Dropout(dropout)\n        self.query = torch.nn.Linear(d_model, d_model)\n        self.key = torch.nn.Linear(d_model, d_model)\n        self.value = torch.nn.Linear(d_model, d_model)\n        self.output_linear = torch.nn.Linear(d_model, d_model)\n\n    def forward(self, query, key, value, mask):\n        batch_size = query.size(0)\n        query = self.query(query).view(batch_size, -1, self.heads, self.d_k).transpose(1, 2)\n        key = self.key(key).view(batch_size, -1, self.heads, self.d_k).transpose(1, 2)\n        value = self.value(value).view(batch_size, -1, self.heads, self.d_k).transpose(1, 2)\n\n        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(query.size(-1))\n        scores = scores.masked_fill(mask == 0, -1e9)\n\n        weights = F.softmax(scores, dim=-1)\n        weights = self.dropout(weights)\n\n        context = torch.matmul(weights, value)\n        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.heads * self.d_k)\n        return self.output_linear(context)\n\nclass BERT(torch.nn.Module):\n    def __init__(self, vocab_size=VOCAB_SIZE, d_model=EMBED_LEN, n_layers=12, heads=12, dropout=0.1):\n        super().__init__()\n        self.embedding = BERTEmbedding(vocab_size, d_model, SEQ_LEN)\n        self.encoder_layers = torch.nn.ModuleList([EncoderLayer(d_model, heads, d_model * 4, dropout) for _ in range(n_layers)])\n\n    def forward(self, x, attention_mask):\n        x = self.embedding(x)\n        mask = attention_mask.unsqueeze(1).unsqueeze(2)\n        for layer in self.encoder_layers:\n            x = layer(x, mask)\n        return x\n        \nclass ClassificationModel(torch.nn.Module):\n    def __init__(self, hidden):\n        super().__init__()\n        # Create separate linear layers for each classification task\n        self.linear1 = torch.nn.Linear(hidden, 2)\n        self.linear2 = torch.nn.Linear(hidden, 2)\n        self.softmax = torch.nn.LogSoftmax(dim=-1)\n    \n    def forward(self, x,task):\n        # Extract tokens\n        if(task==1):\n            token=x[:, 0, :]\n            return self.linear1(token)\n        else:\n            token=x[:, 1, :]\n            return self.linear2(token)\n        # Apply linear layers and find the class indices directly\n        # class_indices = torch.stack([\n        #     torch.argmax(self.linear1(token1), dim=-1),  # Shape: (batch_size,)\n        #     torch.argmax(self.linear2(token2), dim=-1),  # Shape: (batch_size,)\n        #     torch.argmax(self.linear3(token3), dim=-1),  # Shape: (batch_size,)\n        #     torch.argmax(self.linear4(token4), dim=-1)   # Shape: (batch_size,)\n        # ])  # Shape: (4, batch_size)\n\n        # Transpose to get the desired shape (batch_size, 4)\n        # class_indices = class_indices.transpose(0, 1)  # Shape: (batch_size, 4)\n\n        # return class_indices\n        \nclass BERTLM(torch.nn.Module):\n    def __init__(self, bert: BERT, vocab_size):\n        super().__init__()\n        self.bert = bert\n        self.class_lm=ClassificationModel(self.bert.embedding.embed_size)\n\n    def forward(self, x, attention_mask,task):\n        x = self.bert(x, attention_mask)\n        return self.class_lm(x,task)\n\nclass FeedForward(torch.nn.Module):\n    def __init__(self, d_model, middle_dim=2048, dropout=0.1):\n        super(FeedForward, self).__init__()\n        self.fc1 = torch.nn.Linear(d_model, middle_dim)\n        self.fc2 = torch.nn.Linear(middle_dim, d_model)\n        self.dropout = torch.nn.Dropout(dropout)\n        self.activation = torch.nn.GELU()\n\n    def forward(self, x):\n        out = self.activation(self.fc1(x))\n        out = self.fc2(self.dropout(out))\n        return out\n\nclass EncoderLayer(torch.nn.Module):\n    def __init__(self, d_model=EMBED_LEN, heads=12, feed_forward_hidden=EMBED_LEN*4, dropout=0.1):\n        super(EncoderLayer, self).__init__()\n        self.layernorm = torch.nn.LayerNorm(d_model)\n        self.self_multihead = MultiHeadedAttention(heads, d_model)\n        self.feed_forward = FeedForward(d_model, middle_dim=feed_forward_hidden)\n        self.dropout = torch.nn.Dropout(dropout)\n\n    def forward(self, embeddings, mask):\n        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n        interacted = self.layernorm(interacted + embeddings)\n        feed_forward_out = self.dropout(self.feed_forward(interacted))\n        encoded = self.layernorm(feed_forward_out + interacted)\n        return encoded\n\ndef differentiable_f1_loss(logits, labels, epsilon=1e-7):\n    \"\"\"\n    logits: [batch_size, num_classes] (raw model outputs)\n    labels: [batch_size] (ground truth indices)\n    epsilon: Small value to avoid division by zero\n    \"\"\"\n    num_classes = logits.size(1)\n    \n    # Convert labels to one-hot encoding\n    labels_one_hot = F.one_hot(labels, num_classes).float()\n    \n    # Convert logits to probabilities\n    probs = torch.softmax(logits, dim=1)\n    \n    # True positives, false positives, false negatives\n    tp = (probs * labels_one_hot).sum(dim=0)  # True positives for each class\n    fp = ((1 - labels_one_hot) * probs).sum(dim=0)  # False positives for each class\n    fn = (labels_one_hot * (1 - probs)).sum(dim=0)  # False negatives for each class\n    \n    # Calculate F1 for each class\n    f1 = 2 * tp / (2 * tp + fp + fn + epsilon)\n    \n    # Macro F1 Loss (1 - average F1 across all classes)\n    f1_loss = 1 - f1.mean()\n    \n    return f1_loss\n\ndef train_bert(model, train_dataloader, optimizer,scheduler, device, epoch_start,epoch_end):\n    model.train()\n    \n    for epoch in range(epoch_start,epoch_end):\n        loss_list=[]\n        total_loss = 0\n        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\"):\n            optimizer.zero_grad()\n            bert_input = batch['bert_input'].to(device)\n            bert_label = batch['bert_label'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            bert_label=bert_label.transpose(0,1)\n            for i in range(2):\n                outputs = model(bert_input, attention_mask,i)\n                \n                # print('outputs',outputs)\n                # print('labels',bert_label)\n                \n                # loss=F.cross_entropy(outputs,bert_label[i])\n                loss = differentiable_f1_loss(outputs,bert_label[i])\n\n                # Backpropagate\n                loss.backward()\n\n                loss_list.append(loss.item())\n                # f1_loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                optimizer.step()\n                scheduler.step()\n                total_loss += loss.item()\n         \n        avg_loss = total_loss / (len(train_dataloader)*2)\n        print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss:.4f}\",total_loss)\n        # print(loss_list)\n\ndataset = BERTDataset(conv, tokenizer,labels)\ntrain_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n\nprint(f\"Vocabulary size: {tokenizer.get_vocab_size()}\")\nprint(f\"Number of sentences: {len(dataset)}\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\nbert = BERT(vocab_size=VOCAB_SIZE, d_model=EMBED_LEN, n_layers=12, heads=12)\nmodel = BERTLM(bert, vocab_size=VOCAB_SIZE)\nmodel.to(device)\noptimizer = AdamW(model.parameters(), lr=2e-5)\nWARMUP_STEPS=10000\nEPOCHS=20\ntotal_steps = len(train_loader) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=total_steps)","metadata":{"id":"XTthZzTranCU","execution":{"iopub.status.busy":"2024-11-19T05:06:17.615808Z","iopub.execute_input":"2024-11-19T05:06:17.616295Z","iopub.status.idle":"2024-11-19T05:06:17.833107Z","shell.execute_reply.started":"2024-11-19T05:06:17.616261Z","shell.execute_reply":"2024-11-19T05:06:17.832095Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Vocabulary size: 20000\nNumber of sentences: 6396\nUsing device: cuda\ntorch.Size([68, 252])\ndiv_term tensor([1.0000e+00, 9.2951e-01, 8.6399e-01, 8.0309e-01, 7.4648e-01, 6.9386e-01,\n        6.4495e-01, 5.9948e-01, 5.5723e-01, 5.1795e-01, 4.8144e-01, 4.4750e-01,\n        4.1596e-01, 3.8664e-01, 3.5938e-01, 3.3405e-01, 3.1050e-01, 2.8861e-01,\n        2.6827e-01, 2.4936e-01, 2.3178e-01, 2.1544e-01, 2.0026e-01, 1.8614e-01,\n        1.7302e-01, 1.6082e-01, 1.4949e-01, 1.3895e-01, 1.2915e-01, 1.2005e-01,\n        1.1159e-01, 1.0372e-01, 9.6411e-02, 8.9615e-02, 8.3298e-02, 7.7426e-02,\n        7.1969e-02, 6.6895e-02, 6.2180e-02, 5.7797e-02, 5.3723e-02, 4.9936e-02,\n        4.6416e-02, 4.3144e-02, 4.0103e-02, 3.7276e-02, 3.4648e-02, 3.2206e-02,\n        2.9936e-02, 2.7826e-02, 2.5864e-02, 2.4041e-02, 2.2346e-02, 2.0771e-02,\n        1.9307e-02, 1.7946e-02, 1.6681e-02, 1.5505e-02, 1.4412e-02, 1.3396e-02,\n        1.2452e-02, 1.1574e-02, 1.0758e-02, 1.0000e-02, 9.2951e-03, 8.6399e-03,\n        8.0309e-03, 7.4648e-03, 6.9386e-03, 6.4495e-03, 5.9948e-03, 5.5723e-03,\n        5.1795e-03, 4.8144e-03, 4.4750e-03, 4.1596e-03, 3.8664e-03, 3.5938e-03,\n        3.3405e-03, 3.1050e-03, 2.8861e-03, 2.6827e-03, 2.4936e-03, 2.3178e-03,\n        2.1544e-03, 2.0026e-03, 1.8614e-03, 1.7302e-03, 1.6082e-03, 1.4949e-03,\n        1.3895e-03, 1.2915e-03, 1.2005e-03, 1.1159e-03, 1.0372e-03, 9.6411e-04,\n        8.9615e-04, 8.3298e-04, 7.7426e-04, 7.1969e-04, 6.6896e-04, 6.2180e-04,\n        5.7797e-04, 5.3723e-04, 4.9936e-04, 4.6416e-04, 4.3144e-04, 4.0103e-04,\n        3.7276e-04, 3.4648e-04, 3.2206e-04, 2.9936e-04, 2.7826e-04, 2.5864e-04,\n        2.4041e-04, 2.2346e-04, 2.0771e-04, 1.9307e-04, 1.7946e-04, 1.6681e-04,\n        1.5505e-04, 1.4412e-04, 1.3396e-04, 1.2452e-04, 1.1574e-04, 1.0758e-04])\ndiv_term_dim torch.Size([126])\npe s torch.Size([68, 252])\npe tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n          0.0000e+00,  1.0000e+00],\n        [ 8.4147e-01,  5.4030e-01,  8.0133e-01,  ...,  1.0000e+00,\n          1.0758e-04,  1.0000e+00],\n        [ 9.0930e-01, -4.1615e-01,  9.5875e-01,  ...,  1.0000e+00,\n          2.1517e-04,  1.0000e+00],\n        ...,\n        [ 8.2683e-01, -5.6245e-01, -6.6529e-01,  ...,  9.9997e-01,\n          6.9929e-03,  9.9998e-01],\n        [-2.6551e-02, -9.9965e-01, -9.9625e-01,  ...,  9.9997e-01,\n          7.1005e-03,  9.9997e-01],\n        [-8.5552e-01, -5.1777e-01, -5.2669e-01,  ...,  9.9997e-01,\n          7.2080e-03,  9.9997e-01]])\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# tamil_text_files = ['/kaggle/input/wikidata/diamond_merged.txt','/kaggle/input/wikidata/main_train_valid.txt' ]\ntamil_text_files=['/kaggle/input/iconst/data.txt']\ntokenizer = create_tokenizer(tamil_text_files)  ","metadata":{"execution":{"iopub.status.busy":"2024-11-19T05:04:57.033972Z","iopub.execute_input":"2024-11-19T05:04:57.034647Z","iopub.status.idle":"2024-11-19T05:04:57.728014Z","shell.execute_reply.started":"2024-11-19T05:04:57.034609Z","shell.execute_reply":"2024-11-19T05:04:57.726916Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\n\n\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"train_bert(model,train_loader,optimizer,scheduler,device,0,50) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T05:06:27.521195Z","iopub.execute_input":"2024-11-19T05:06:27.522108Z","iopub.status.idle":"2024-11-19T05:24:16.113825Z","shell.execute_reply.started":"2024-11-19T05:06:27.522069Z","shell.execute_reply":"2024-11-19T05:24:16.112859Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 200/200 [00:21<00:00,  9.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Average Loss: 0.5105 204.20457434654236\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 200/200 [00:21<00:00,  9.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Average Loss: 0.5036 201.45883905887604\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 200/200 [00:21<00:00,  9.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Average Loss: 0.5039 201.5770139694214\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 200/200 [00:21<00:00,  9.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Average Loss: 0.5039 201.57538855075836\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 200/200 [00:21<00:00,  9.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Average Loss: 0.5004 200.14226204156876\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|██████████| 200/200 [00:21<00:00,  9.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6, Average Loss: 0.4975 199.01659244298935\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|██████████| 200/200 [00:21<00:00,  9.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7, Average Loss: 0.4962 198.46813625097275\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8: 100%|██████████| 200/200 [00:21<00:00,  9.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8, Average Loss: 0.4896 195.8469591140747\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9: 100%|██████████| 200/200 [00:21<00:00,  9.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9, Average Loss: 0.4851 194.04608988761902\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10: 100%|██████████| 200/200 [00:21<00:00,  9.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10, Average Loss: 0.4764 190.5736141204834\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11: 100%|██████████| 200/200 [00:21<00:00,  9.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11, Average Loss: 0.4752 190.0696723461151\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12: 100%|██████████| 200/200 [00:21<00:00,  9.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12, Average Loss: 0.4661 186.43098187446594\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13: 100%|██████████| 200/200 [00:21<00:00,  9.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13, Average Loss: 0.4630 185.1801819205284\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14: 100%|██████████| 200/200 [00:21<00:00,  9.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14, Average Loss: 0.4558 182.32910656929016\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15: 100%|██████████| 200/200 [00:21<00:00,  9.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15, Average Loss: 0.4574 182.94103956222534\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16: 100%|██████████| 200/200 [00:21<00:00,  9.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16, Average Loss: 0.4548 181.90609723329544\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17: 100%|██████████| 200/200 [00:21<00:00,  9.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17, Average Loss: 0.4478 179.11536103487015\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18: 100%|██████████| 200/200 [00:21<00:00,  9.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18, Average Loss: 0.4525 181.01167118549347\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19: 100%|██████████| 200/200 [00:21<00:00,  9.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19, Average Loss: 0.4604 184.16417688131332\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20: 100%|██████████| 200/200 [00:21<00:00,  9.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20, Average Loss: 0.4582 183.27955704927444\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21: 100%|██████████| 200/200 [00:21<00:00,  9.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21, Average Loss: 0.4476 179.04132109880447\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22: 100%|██████████| 200/200 [00:21<00:00,  9.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22, Average Loss: 0.4701 188.029955804348\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23: 100%|██████████| 200/200 [00:21<00:00,  9.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23, Average Loss: 0.4531 181.229731798172\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24: 100%|██████████| 200/200 [00:21<00:00,  9.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24, Average Loss: 0.4647 185.89732521772385\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25: 100%|██████████| 200/200 [00:21<00:00,  9.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25, Average Loss: 0.4688 187.5254538655281\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26: 100%|██████████| 200/200 [00:21<00:00,  9.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26, Average Loss: 0.4382 175.26304960250854\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27: 100%|██████████| 200/200 [00:21<00:00,  9.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27, Average Loss: 0.4381 175.22426879405975\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28: 100%|██████████| 200/200 [00:21<00:00,  9.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28, Average Loss: 0.4392 175.6777145266533\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29: 100%|██████████| 200/200 [00:21<00:00,  9.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29, Average Loss: 0.4378 175.11579364538193\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30: 100%|██████████| 200/200 [00:21<00:00,  9.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30, Average Loss: 0.4406 176.2549278140068\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31: 100%|██████████| 200/200 [00:21<00:00,  9.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 31, Average Loss: 0.4394 175.74333381652832\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32: 100%|██████████| 200/200 [00:21<00:00,  9.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 32, Average Loss: 0.4379 175.14349806308746\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33: 100%|██████████| 200/200 [00:21<00:00,  9.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 33, Average Loss: 0.4395 175.8140271306038\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34: 100%|██████████| 200/200 [00:21<00:00,  9.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 34, Average Loss: 0.4345 173.79201596975327\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35: 100%|██████████| 200/200 [00:21<00:00,  9.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 35, Average Loss: 0.4405 176.20951128005981\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36: 100%|██████████| 200/200 [00:21<00:00,  9.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 36, Average Loss: 0.4412 176.4751180410385\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37: 100%|██████████| 200/200 [00:21<00:00,  9.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 37, Average Loss: 0.4365 174.61115664243698\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38: 100%|██████████| 200/200 [00:21<00:00,  9.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 38, Average Loss: 0.4344 173.74956411123276\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39: 100%|██████████| 200/200 [00:21<00:00,  9.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 39, Average Loss: 0.4355 174.20379787683487\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40: 100%|██████████| 200/200 [00:21<00:00,  9.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 40, Average Loss: 0.4363 174.516526222229\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41: 100%|██████████| 200/200 [00:21<00:00,  9.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 41, Average Loss: 0.4376 175.053701877594\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42: 100%|██████████| 200/200 [00:21<00:00,  9.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 42, Average Loss: 0.4354 174.1732148528099\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43: 100%|██████████| 200/200 [00:21<00:00,  9.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 43, Average Loss: 0.4353 174.1070762872696\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44: 100%|██████████| 200/200 [00:21<00:00,  9.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 44, Average Loss: 0.4388 175.53546780347824\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45: 100%|██████████| 200/200 [00:21<00:00,  9.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 45, Average Loss: 0.4337 173.4681943655014\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46: 100%|██████████| 200/200 [00:21<00:00,  9.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 46, Average Loss: 0.4351 174.0354307293892\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47: 100%|██████████| 200/200 [00:21<00:00,  9.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 47, Average Loss: 0.4422 176.862455368042\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48: 100%|██████████| 200/200 [00:21<00:00,  9.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 48, Average Loss: 0.4438 177.5153232216835\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49: 100%|██████████| 200/200 [00:21<00:00,  9.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 49, Average Loss: 0.4334 173.3761087656021\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50: 100%|██████████| 200/200 [00:21<00:00,  9.38it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 50, Average Loss: 0.4336 173.42137491703033\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"loss_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T05:24:26.024435Z","iopub.execute_input":"2024-11-19T05:24:26.024853Z","iopub.status.idle":"2024-11-19T05:24:26.101046Z","shell.execute_reply.started":"2024-11-19T05:24:26.024820Z","shell.execute_reply":"2024-11-19T05:24:26.100014Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloss_list\u001b[49m\n","\u001b[0;31mNameError\u001b[0m: name 'loss_list' is not defined"],"ename":"NameError","evalue":"name 'loss_list' is not defined","output_type":"error"}],"execution_count":24},{"cell_type":"raw","source":"   ","metadata":{"execution":{"iopub.status.busy":"2024-11-18T11:56:46.424110Z","iopub.execute_input":"2024-11-18T11:56:46.424800Z","iopub.status.idle":"2024-11-18T11:56:46.465673Z","shell.execute_reply.started":"2024-11-18T11:56:46.424763Z","shell.execute_reply":"2024-11-18T11:56:46.464532Z"}}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"def load_model(load_directory,model):\n    model_path = os.path.join(load_directory, \"bert_model_17_epoch.pth\")\n    print(f\"Model and vocabulary loaded from {load_directory}\")\n    model.load_state_dict(torch.load(model_path))\n    return model\nmodel=load_model(\"/kaggle/input/bert/pytorch/abc/1\", model)","metadata":{"execution":{"iopub.status.busy":"2024-09-24T16:34:22.902706Z","iopub.execute_input":"2024-09-24T16:34:22.903237Z","iopub.status.idle":"2024-09-24T16:34:23.941653Z","shell.execute_reply.started":"2024-09-24T16:34:22.903193Z","shell.execute_reply":"2024-09-24T16:34:23.940579Z"}}},{"cell_type":"code","source":"# def load_model(load_directory,model): \n#     model_path = os.path.join(load_directory, \"bert_model_24_epoch.pth\") \n#     print(f\"Model and vocabulary loaded from {load_directory}\") \n#     model.load_state_dict(torch.load(model_path)) \n#     return model \n# model=load_model(\"/kaggle/input/abcd/pytorch/default/1/\", model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.bert","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.bert.embedding.position.pe.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_embedding=model.bert.embedding.token.weight\nvocab = tokenizer.get_vocab()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"extract embedding from bert","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\ndef get_token_embeddings_custom_bert(model, input_ids):\n    print(input_ids)\n    with torch.no_grad():\n        embeddings = model.bert.embedding(input_ids) \n        for layer in model.bert.encoder_layers:\n            embeddings = layer(embeddings)  \n    \n    return embeddings\n\ndef get_embedding(model, tokenizer, word, top_n=10):\n    dataset = BERTDataset([word], tokenizer)\n    data=dataset.__getitem__(0,flag=False)\n    input_ids = data['bert_input'].unsqueeze(0).to(device)\n    attention_mask = data['attention_mask'].unsqueeze(0).to(device)\n    masked_lm_labels = data['bert_label'].unsqueeze(0).to(device)  \n    \n    word_embeddings = get_token_embeddings_custom_bert(model, input_ids)\n    word_embedding = torch.mean(word_embeddings, dim=1).squeeze(0)\n\n    # Now compute cosine similarity between this word's embedding and all token embeddings\n    all_embeddings = model.bert.embedding.token.weight \n    cosine_similarities = F.cosine_similarity(word_embedding.unsqueeze(0), all_embeddings, dim=1)\n\n    # Get the top N most similar tokens (excluding the queried word itself)\n    top_indices = torch.argsort(cosine_similarities, descending=True)[1:top_n+1]\n    \n    # Convert the token indices back to words\n    similar_words = [tokenizer.convert_ids_to_tokens(idx.item()) for idx in top_indices]\n    \n    print(f\"Top {top_n} words similar to '{word}': {similar_words}\")\n\nword = \"குடியரசு\"\nget_embedding(model, tokenizer, word, top_n=10)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"find_similar_words('முதல',tokenizer,model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"special_tokens = [\"[PAD]\", \"[MASK]\", \"[UNK]\"]\nvocab=tokenizer.get_vocab()\nlen(vocab)\nfor i in special_tokens:\n    print(vocab[i])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"key_int_value_word={}\nfor key,value in vocab.items():\n    key_int_value_word[value]=key","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"check the unknown count in the built tensors for the sentence input","metadata":{}},{"cell_type":"code","source":"# count=0\n# for i in range(80000,85000):\n#     example = dataset[i]\n# #     print(f\"Example {i + 1}:\")\n#     a= example['bert_input']\n# #     print(\"BERT Label Tokens: \", example['bert_label'])\n# #     print(\"Attention Mask: \", example['attention_mask'])\n# # #     print() \n#     a=a.tolist()\n# #     l=[]\n#     for j in a:\n# #         l.append(key_int_value_word[j])\n#         if j==2:\n#             count+=1\n#         if j==0:\n#             break\n#     if i%1000==0:\n#             print(i)\n# #         print(j,end=' ')\n# #     print()\n# #     print(l)\n# print(count)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vocab=tokenizer.get_vocab()\n# sorted_vocab = dict(sorted(vocab.items(), key=lambda item: item[1]))\n\n# with open('output_wiki_test1.txt','w',encoding='utf-8') as f:\n#     for key,value in sorted_vocab.items():\n#         f.write(f'{key}\\t\\t{value} \\n')   ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef test_bert_model(model, tokenizer, sentence, device,target,top_n=5):\n    dataset = BERTDataset([sentence], tokenizer)\n    dataset_target=BERTDataset([target],tokenizer)\n    data_target=dataset_target.__getitem__(0,False)\n    data=dataset.__getitem__(0,False)\n#     print(data)\n#     print(data_target)\n    target_list_tokenized=[]\n    target_input_ids=data_target['bert_input'].unsqueeze(0).to(device)\n    target_list=target_input_ids.tolist()\n    \n    for i in target_list[0]:\n        if i==0:\n            break\n        else:\n            target_list_tokenized.append(i)\n    #print('target',target_list_tokenized)\n    input_ids = data['bert_input'].unsqueeze(0).to(device)\n    #print(input_ids)\n    attention_mask = data['attention_mask'].unsqueeze(0).to(device)\n    \n    key_indexes=[input_ids.tolist()[0].index(i) for i in target_list_tokenized ]\n    print(key_indexes)\n    \n    #print(input_ids.shape)\n    model.eval()\n    predictions=[]\n    with torch.no_grad():\n        outputs=model.bert(input_ids, attention_mask)\n        #print('out',outputs.shape)\n    \n    context_embedding_list=[]\n    output=outputs.tolist()\n    for i in key_indexes:\n        context_embedding_list.append(output[0][i])\n    context_embedding=[0]*252\n    for context in context_embedding_list:\n        context_embedding = [context_embedding[i] + context[i] for i in range(len(context))]\n   # print(len(context_embedding))\n    context_embedding_tensor = torch.tensor(context_embedding, dtype=torch.float32).to(device)\n    #print(context_embedding_tensor)\n    all_embeddings = model.bert.embedding.token.weight \n    cosine_similarities = F.cosine_similarity(context_embedding_tensor.unsqueeze(0), all_embeddings, dim=1)\n\n    # Get the top N most similar tokens (excluding the queried word itself)\n    top_indices = torch.argsort(cosine_similarities, descending=True)[1:10]\n    print('top',top_indices)\n    # Convert the token indices back to words\n    return [tokenizer.id_to_token(idx.item()) for idx in top_indices]\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_sentence = \"அகர முதல எழுத்தெல்லாம் ஆதி பகவன் முதற்றே உலகு\"\nprint(test_bert_model(model, tokenizer, test_sentence, device,\"அகர\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\ndef predict_masked_words(model, tokenizer, sentence, device):\n    model.eval()\n    tokens = tokenizer.encode(sentence).ids\n    print('tokens',tokens)\n    mask_token_id = tokenizer.token_to_id('[MASK]')\n    print('mask token id',mask_token_id)\n    mask_positions = [i for i, token in enumerate(tokens) if token == mask_token_id]\n    print('mask_position',mask_positions)\n    input_ids = torch.tensor([tokens]).to(device)\n    print('input ids',input_ids)\n    attention_mask = torch.ones_like(input_ids).to(device)\n    print('att',attention_mask)\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask)\n    print('outputs',outputs.shape)\n    predictions = []\n    for pos in range(10):\n        predicted_id = outputs[0, pos].argmax().item()\n        predicted_token = tokenizer.id_to_token(predicted_id)\n        predictions.append((pos, predicted_token))\n    \n    return predictions\n\n# def analyze_training_performance(losses, epochs):\n#     import matplotlib.pyplot as plt\n    \n#     plt.figure(figsize=(10, 6))\n#     plt.plot(range(1, epochs+1), losses)\n#     plt.title('Training Loss over Epochs')\n#     plt.xlabel('Epoch')\n#     plt.ylabel('Loss')\n#     plt.grid(True)\n#     plt.show()\n    \n#     print(f\"Initial loss: {losses[0]:.4f}\")\n#     print(f\"Final loss: {losses[-1]:.4f}\")\n#     print(f\"Absolute improvement: {losses[0] - losses[-1]:.4f}\")\n#     print(f\"Relative improvement: {(losses[0] - losses[-1]) / losses[0] * 100:.2f}%\")\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\ntest_sentence = \"அகர [MASK] எழுத்தெல்லாம் ஆதி [MASK] முதற்றே உலகு\"\npredictions = predict_masked_words(model, tokenizer, test_sentence, device)\n\nprint(\"Original sentence:\", test_sentence)\nprint(\"Predictions:\")\nfor pos, pred in predictions:\n    print(f\"Position {pos}: {pred}\")\n\n# # Assuming you've collected losses during training\n# training_losses = [...]  # List of loss values from each epoch\n# analyze_training_performance(training_losses, len(training_losses))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# torch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# save_directory = \"../models/diamond_merged_tokenized\"\n# save_model(model, tokenizer, save_directory)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef load_model(load_directory,model):\n    model_path = os.path.join(load_directory, \"bert_model11th_epoch.pth\")\n#     bert = BERT(vocab_size=vocab_size, d_model=d_model, n_layers=n_layers, heads=heads)\n#     model = BERTLM(bert, vocab_size=vocab_size)\n    print(f\"Model and vocabulary loaded from {load_directory}\")\n    model.load_state_dict(torch.load(model_path))\n    return model\n#     vocab_path = os.path.join(load_directory, \"vocab.json\")\n#     with open(vocab_path, 'r', encoding='utf-8') as f:\n#         vocab = json.load(f)\n    \n#     tokenizer = Tokenizer(models.WordLevel(vocab=vocab, unk_token=\"[UNK]\"))\n#     tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n    \n#     special_tokens = [\"[PAD]\",\"[MASK]\", \"[UNK]\"]\n#     tokenizer.add_special_tokens(special_tokens)\n\n#     tokenizer.post_processor = processors.TemplateProcessing(\n#         single=\"[CLS] $A [SEP]\",\n#         pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n#         special_tokens=[\n#             (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n#             (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n#         ],\n#     )\n    \n   \n#     return model, tokenizer\nmodel=load_model(\"/kaggle/input/bert/pytorch/default/1\", model)\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# loaded_model.to(device)\n\n# sentence = \"கற்றதனால் ஆய எல்லா பயன் என்கொல் நூல்களையும் கற்றவர்க்கு அக்கல்வி அறிவினாலாய பயன் யாது\"\n# word = \"என்கொல்\"\n# similar_words = find_contextual_similar_words(sentence, word, tokenizer, model, device)\n# print(f\"Words similar to '{word}' in the context of '{sentence}': {similar_words}\")\n\n# masked_sentence = \"கற்றதனால் [MASK] பயன் என்கொல் \"\n# predicted_word, predicted_sentence = predict_masked_word(masked_sentence, tokenizer, model, device)\n# print(f\"Original masked sentence: {masked_sentence}\")\n# print(f\"Predicted word: {predicted_word}\")\n# print(f\"Predicted sentence: {predicted_sentence}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef find_contextual_similar_words(sentence, word, tokenizer, model, device, top_n=5):\n    model.eval()\n    with torch.no_grad():\n        tokens = [tokenizer.token_to_id(token) for token in sentence.split()]\n        word_id = tokenizer.token_to_id(word)\n        if word_id is None or word_id not in tokens:\n            return []\n        word_position = tokens.index(word_id)\n        print(tokens)\n        input_ids = torch.tensor([tokens]).to(device)\n        attention_mask = torch.ones_like(input_ids).to(device)\n        outputs = model.bert(input_ids, attention_mask)\n        contextual_embedding = outputs[0, word_position]\n        all_embeddings = model.bert.embedding.token.weight\n        cosine_similarities = F.cosine_similarity(contextual_embedding, all_embeddings)\n        top_indices = torch.argsort(cosine_similarities, descending=True)[1:top_n+1]\n        similar_words = [tokenizer.id_to_token(idx.item()) for idx in top_indices]\n        \n        return similar_words\n\nsentence = \"வானூர்தி நிலையங்கள் உள்ளன\"\nword = \"வானூர்தி\"\nsimilar_words = find_contextual_similar_words(sentence, word, tokenizer, model, device)\nprint(f\"Words similar to '{word}' in the context of '{sentence}': {similar_words}\") ","metadata":{"id":"tvuR898Z8ZkK","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"***claude code***","metadata":{"id":"fr4T99aD8XwC"}},{"cell_type":"code","source":"","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"duDDjVH2_3Dp","outputId":"4917c691-489b-4b7b-e2b1-837f5e68f114","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with torch.no_grad():\n        word_id = tokenizer.token_to_id(\"விரிநீர்\")\n        if word_id is None:\n            print('None')\n        word_embedding = model.bert.embedding.token.weight[word_id]\n        cosine_similarities = F.cosine_similarity(word_embedding, model.bert.embedding.token.weight)\n        top_indices = torch.argsort(cosine_similarities, descending=True)[1:5]\n        print([tokenizer.id_to_token(idx.item()) for idx in top_indices])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xZnSzTyREp2X","outputId":"c3b59dba-ef85-4f7f-f79e-73a155ca3778","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def print_vocabulary(tokenizer):\n#     vocab = tokenizer.get_vocab()\n#     sorted_vocab=sorted(vocab.items(), key=lambda x: x[1])\n#     print(\"Vocabulary:\")\n#     for word, index in sorted_vocab:\n#         print(word,end=' ')\n#     print(f\"Total vocabulary size: {len(vocab)}\")\n","metadata":{"id":"ZWbvmAh2E6H_","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
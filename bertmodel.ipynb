{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9448963,"sourceType":"datasetVersion","datasetId":5743086},{"sourceId":9941909,"sourceType":"datasetVersion","datasetId":6112794},{"sourceId":118881,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":99966,"modelId":124137},{"sourceId":119356,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":100370,"modelId":124137},{"sourceId":120158,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":101071,"modelId":125250}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport torch\nimport re\nimport random\nimport torch\nfrom torch.utils.data import Dataset\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordPiece\nfrom tokenizers.pre_tokenizers import Whitespace\nfrom tokenizers.trainers import WordPieceTrainer\nimport random\nfrom tokenizers import BertWordPieceTokenizer\nfrom transformers import BertTokenizer\nimport tqdm\nfrom torch.utils.data import Dataset, DataLoader\nimport itertools\nimport math\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch.optim import Adam","metadata":{"id":"Z_p8twHVZDLx","execution":{"iopub.status.busy":"2024-11-19T03:50:09.132991Z","iopub.execute_input":"2024-11-19T03:50:09.133496Z","iopub.status.idle":"2024-11-19T03:50:09.139059Z","shell.execute_reply.started":"2024-11-19T03:50:09.133462Z","shell.execute_reply":"2024-11-19T03:50:09.138232Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"merge= r'/kaggle/input/wikidata/diamond_merged.txt' # Parimelagar urai with its explanation and  Random Merge 20 times\n# train = r'../datasets/main_train_train.txt'\ntest = r'/kaggle/input/wikidata/main_train_valid.txt' # total 400MB\niconst=r'/kaggle/input/iconst/data.txt'","metadata":{"id":"_-7ywb1aZcNn","execution":{"iopub.status.busy":"2024-11-19T03:50:13.988948Z","iopub.execute_input":"2024-11-19T03:50:13.989278Z","iopub.status.idle":"2024-11-19T03:50:13.993542Z","shell.execute_reply.started":"2024-11-19T03:50:13.989248Z","shell.execute_reply":"2024-11-19T03:50:13.992743Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"conv=[]\n# with open( merge, 'r',encoding='utf-8') as c:\n#     merged = c.readlines()           \n# # with open( train, 'r',encoding='utf-8') as c:\n# #     trained = c.readlines()\n# with open( test, 'r',encoding='utf-8') as c:\n#     tested = c.readlines()\nwith open( iconst, 'r',encoding='utf-8') as c:\n    icst = c.readlines()  \nconv=icst","metadata":{"id":"qcAna19JZtnb","execution":{"iopub.status.busy":"2024-11-19T03:50:17.426286Z","iopub.execute_input":"2024-11-19T03:50:17.427076Z","iopub.status.idle":"2024-11-19T03:50:17.458658Z","shell.execute_reply.started":"2024-11-19T03:50:17.427045Z","shell.execute_reply":"2024-11-19T03:50:17.458005Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# conv=tested+merged\n","metadata":{"execution":{"iopub.status.busy":"2024-11-18T13:03:48.927996Z","iopub.execute_input":"2024-11-18T13:03:48.928388Z","iopub.status.idle":"2024-11-18T13:03:48.932690Z","shell.execute_reply.started":"2024-11-18T13:03:48.928351Z","shell.execute_reply":"2024-11-18T13:03:48.931731Z"},"trusted":true},"outputs":[],"execution_count":27},{"cell_type":"code","source":"len(conv)#,len(trained),len(tested) Total  sentences","metadata":{"execution":{"iopub.status.busy":"2024-11-18T14:24:09.162095Z","iopub.execute_input":"2024-11-18T14:24:09.162379Z","iopub.status.idle":"2024-11-18T14:24:09.168766Z","shell.execute_reply.started":"2024-11-18T14:24:09.162349Z","shell.execute_reply":"2024-11-18T14:24:09.167839Z"},"trusted":true},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"6396"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"sum=0\nl=0\ndic={}\nword_freq_count={}\ns=set()\nfor sent in conv:\n    length=len(sent.split())\n    l=max(l,length)\n    for word in sent.split():\n        if word in word_freq_count.keys():\n            word_freq_count[word]=word_freq_count[word]+1\n        else:\n            word_freq_count[word]=1\n        s.add(word)\n    dic[length]=1 if length not in dic.keys() else dic[length]+1\nlen(s),l                                                           ","metadata":{"execution":{"iopub.status.busy":"2024-11-19T03:50:22.668136Z","iopub.execute_input":"2024-11-19T03:50:22.668501Z","iopub.status.idle":"2024-11-19T03:50:22.772949Z","shell.execute_reply.started":"2024-11-19T03:50:22.668469Z","shell.execute_reply":"2024-11-19T03:50:22.772166Z"},"trusted":true},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(22324, 233)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"word_freq_count_above_10={}\nfor key,value in word_freq_count.items():\n    if value>10:\n        word_freq_count_above_10[key]=value\nprint(len(word_freq_count_above_10))","metadata":{"execution":{"iopub.status.busy":"2024-11-19T03:50:26.498114Z","iopub.execute_input":"2024-11-19T03:50:26.498516Z","iopub.status.idle":"2024-11-19T03:50:26.506413Z","shell.execute_reply.started":"2024-11-19T03:50:26.498481Z","shell.execute_reply":"2024-11-19T03:50:26.505536Z"},"trusted":true},"outputs":[{"name":"stdout","text":"1769\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"sorted_dict = dict(sorted(dic.items()))\nsorted_dict    #sentence Length","metadata":{"execution":{"iopub.status.busy":"2024-11-19T03:50:29.462869Z","iopub.execute_input":"2024-11-19T03:50:29.463208Z","iopub.status.idle":"2024-11-19T03:50:29.470610Z","shell.execute_reply.started":"2024-11-19T03:50:29.463178Z","shell.execute_reply":"2024-11-19T03:50:29.469703Z"},"trusted":true},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{0: 2,\n 1: 24,\n 2: 72,\n 3: 93,\n 4: 109,\n 5: 122,\n 6: 128,\n 7: 142,\n 8: 142,\n 9: 141,\n 10: 129,\n 11: 129,\n 12: 135,\n 13: 141,\n 14: 126,\n 15: 141,\n 16: 130,\n 17: 136,\n 18: 151,\n 19: 152,\n 20: 148,\n 21: 109,\n 22: 142,\n 23: 128,\n 24: 134,\n 25: 122,\n 26: 118,\n 27: 133,\n 28: 126,\n 29: 84,\n 30: 112,\n 31: 116,\n 32: 102,\n 33: 94,\n 34: 122,\n 35: 98,\n 36: 105,\n 37: 93,\n 38: 120,\n 39: 106,\n 40: 122,\n 41: 112,\n 42: 150,\n 43: 164,\n 44: 145,\n 45: 174,\n 46: 161,\n 47: 140,\n 48: 105,\n 49: 93,\n 50: 95,\n 51: 55,\n 52: 41,\n 53: 36,\n 54: 27,\n 55: 14,\n 56: 12,\n 57: 12,\n 58: 7,\n 59: 7,\n 60: 5,\n 61: 3,\n 62: 3,\n 63: 1,\n 65: 3,\n 66: 1,\n 67: 2,\n 69: 3,\n 70: 2,\n 71: 4,\n 72: 3,\n 73: 1,\n 75: 2,\n 77: 1,\n 78: 1,\n 80: 1,\n 81: 1,\n 82: 2,\n 83: 1,\n 85: 1,\n 87: 2,\n 90: 1,\n 92: 1,\n 94: 1,\n 96: 1,\n 97: 1,\n 99: 2,\n 104: 1,\n 105: 1,\n 107: 1,\n 109: 1,\n 114: 1,\n 116: 1,\n 118: 1,\n 122: 1,\n 131: 1,\n 132: 1,\n 151: 1,\n 156: 1,\n 159: 1,\n 161: 1,\n 165: 1,\n 180: 1,\n 233: 1}"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"process labels","metadata":{}},{"cell_type":"code","source":"import json\n\nwith open(r'/kaggle/input/iconst/labels.json', 'r') as f:\n    data_v = json.load(f)\n\nlabels = [data_v[key] for key in sorted(data_v.keys())]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T03:50:34.988812Z","iopub.execute_input":"2024-11-19T03:50:34.989149Z","iopub.status.idle":"2024-11-19T03:50:35.010426Z","shell.execute_reply.started":"2024-11-19T03:50:34.989118Z","shell.execute_reply":"2024-11-19T03:50:35.009595Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels[:10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T03:50:42.807690Z","iopub.execute_input":"2024-11-19T03:50:42.808309Z","iopub.status.idle":"2024-11-19T03:50:42.814382Z","shell.execute_reply.started":"2024-11-19T03:50:42.808276Z","shell.execute_reply":"2024-11-19T03:50:42.813595Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"[['0', '0', '0', '0'],\n ['1', '1', '2', '3'],\n ['1', '0', '1', '1'],\n ['1', '1', '2', '1'],\n ['1', '0', '1', '1'],\n ['0', '0', '0', '0'],\n ['0', '0', '0', '0'],\n ['1', '1', '2', '1'],\n ['0', '0', '0', '0'],\n ['1', '0', '2', '2']]"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# import string\n# def remove_punctuation(sentences):\n#     translator = str.maketrans('', '', string.punctuation)\n#     cleaned_sentences = []\n#     for sentence in sentences:\n#         words = sentence.translate(translator).split()\n#         cleaned_sentence = \" \".join(words)\n#         cleaned_sentences.append(cleaned_sentence)\n    \n#     return cleaned_sentences","metadata":{"execution":{"iopub.status.busy":"2024-09-22T07:43:53.109509Z","iopub.execute_input":"2024-09-22T07:43:53.109929Z","iopub.status.idle":"2024-09-22T07:43:53.114423Z","shell.execute_reply.started":"2024-09-22T07:43:53.109891Z","shell.execute_reply":"2024-09-22T07:43:53.113443Z"},"trusted":true},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# # # conv=trained\n# train_cleaned=remove_punctuation(trained)\n# test_cleaned=remove_punctuation(tested)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T07:43:54.068350Z","iopub.execute_input":"2024-09-22T07:43:54.069302Z","iopub.status.idle":"2024-09-22T07:43:54.072990Z","shell.execute_reply.started":"2024-09-22T07:43:54.069259Z","shell.execute_reply":"2024-09-22T07:43:54.071956Z"},"trusted":true},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# with open('main_train_train.txt','a',encoding='utf-8') as f:\n#     for line in train_cleaned:\n#         if len(line.split())<=64: \n#             f.writelines(f'{line}\\n')\n# with open('main_train_valid.txt','a',encoding='utf-8') as f:\n#     for line in test_cleaned:\n#         if len(line.split())<=64: \n#             f.writelines(f'{line}\\n')\n    ","metadata":{"execution":{"iopub.status.busy":"2024-09-22T07:43:54.487900Z","iopub.execute_input":"2024-09-22T07:43:54.488806Z","iopub.status.idle":"2024-09-22T07:43:54.492824Z","shell.execute_reply.started":"2024-09-22T07:43:54.488764Z","shell.execute_reply":"2024-09-22T07:43:54.491823Z"},"trusted":true},"outputs":[],"execution_count":38},{"cell_type":"code","source":"conv[:5]","metadata":{"execution":{"iopub.status.busy":"2024-11-18T13:03:41.358948Z","iopub.execute_input":"2024-11-18T13:03:41.359804Z","iopub.status.idle":"2024-11-18T13:03:41.365420Z","shell.execute_reply.started":"2024-11-18T13:03:41.359761Z","shell.execute_reply":"2024-11-18T13:03:41.364514Z"},"trusted":true},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"['USER Abhi tak 2000 ke note me mujhe GPS nano chip nahin mila\\n',\n 'USER USER Abe katiye tumse kuch huaa toh jata nahi bas bakri jaise maymay karte rehte ho behan ke lodo tumhari Tablighi jamaat ke karan corona faila aur Bhoomi Poojan mein sab ko screen aur full medical jach ke saath huaa bhosdiwalo\\n',\n 'USER Ye sab sazish hai bina saman ke koi kaise apne gher ja sakta hai dekh lena inn logo ke beech bahut aise jamaat ke wo corona honge jo delhi se nikle they\\n',\n 'abe jao tum to dasko pahle hi fash gye the jab tere dada ne talwar ke nokh par salwar pahena tha tera daram kabhi hindu kabhi muslim\\n',\n 'Ab ye afbah kaun faila Raha hai ki Shahhen bag ke Biryani me Population control ki Dawa Milli Hui hai\\n']"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"# conv=trained\n# print(len(conv),len(trained))","metadata":{"execution":{"iopub.status.busy":"2024-09-22T07:43:55.713154Z","iopub.execute_input":"2024-09-22T07:43:55.713541Z","iopub.status.idle":"2024-09-22T07:43:55.717982Z","shell.execute_reply.started":"2024-09-22T07:43:55.713504Z","shell.execute_reply":"2024-09-22T07:43:55.717019Z"},"trusted":true},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# paths=['/kaggle/input/wikidata/diamond_merged.txt','/kaggle/input/wikidata/main_train_valid.txt' ]\npaths=['/kaggle/input/iconst/data.txt']","metadata":{"id":"s-l8YYCfc3pp","execution":{"iopub.status.busy":"2024-11-19T03:50:50.678559Z","iopub.execute_input":"2024-11-19T03:50:50.678892Z","iopub.status.idle":"2024-11-19T03:50:50.683086Z","shell.execute_reply.started":"2024-11-19T03:50:50.678860Z","shell.execute_reply":"2024-11-19T03:50:50.682134Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom tokenizers import Tokenizer, models, pre_tokenizers, processors\nimport random\nimport math\nimport torch.nn.functional as F\nfrom torch.optim import Adam\nfrom tqdm import tqdm\nfrom tokenizers import trainers\nfrom tokenizers import Tokenizer, models, trainers, pre_tokenizers, processors\nimport torch\nfrom torch.utils.data import Dataset\nimport random\nimport json\nfrom torch.optim import AdamW\nfrom transformers import get_linear_schedule_with_warmup\n\nSEQ_LEN = 68\nEMBED_LEN = 252\nVOCAB_SIZE = 20000   #vocab size\n\ndef create_tokenizer(paths, vocab_size=VOCAB_SIZE):\n    tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n    tokenizer.pre_tokenizer = Whitespace()\n    special_tokens = [\"[CLS1]\",\"[CLS2]\",\"[PAD]\", \"[MASK]\", \"[UNK]\"]\n    tokenizer.add_special_tokens(special_tokens)\n\n    def batch_iterator(file_paths, batch_size=64):\n        for path in file_paths:\n            with open(path, 'r', encoding='utf-8') as f:\n                for line in f:\n                    yield line.strip()\n\n    trainer = WordPieceTrainer(\n        vocab_size=vocab_size,\n        special_tokens=special_tokens\n    )\n    tokenizer.train_from_iterator(batch_iterator(paths), trainer=trainer)\n    \n    return tokenizer\n\nclass BERTDataset(Dataset):\n    def __init__(self, sentences, tokenizer, labels,seq_len=SEQ_LEN):\n        self.tokenizer = tokenizer\n        self.seq_len = seq_len\n        self.sentences = sentences\n        self.labels=labels\n\n    def __len__(self):\n        return len(self.sentences)\n\n    def __getitem__(self, item,flag=False):\n        sentence = self.sentences[item]\n        encoding = self.tokenizer.encode(sentence)\n        \n        # List of special class tokens\n        classes = [\n            self.tokenizer.token_to_id('[CLS1]'),\n            self.tokenizer.token_to_id('[CLS2]'),\n            # self.tokenizer.token_to_id('[CLS3]'),\n            # self.tokenizer.token_to_id('[CLS4]')\n        ]\n        \n        tokens = classes + encoding.ids  \n        if len(tokens) > self.seq_len:\n            tokens = tokens[:self.seq_len]\n        \n        padding_length = self.seq_len - len(tokens)\n        tokens = tokens + [self.tokenizer.token_to_id('[PAD]')] * padding_length\n        \n        mask = [1 if token != self.tokenizer.token_to_id('[PAD]') else 0 for token in tokens]\n        \n        bert_input = tokens.copy()\n        bert_label = [int(self.labels[item][0]), int(self.labels[item][1]), int(self.labels[item][2]), int(self.labels[item][3])]\n    \n        if flag:\n            for i in range(len(tokens)):\n                if mask[i] == 1 and random.random() < 0.25:\n                    if random.random() < 0.8:\n                        bert_input[i] = self.tokenizer.token_to_id('[MASK]')\n                    elif random.random() < 0.5:\n                        bert_input[i] = random.randint(0, self.tokenizer.get_vocab_size() - 1)\n        \n        return {\n            'bert_input': torch.tensor(bert_input),\n            'bert_label': torch.tensor(bert_label),\n            'attention_mask': torch.tensor(mask)\n        }\n\n    \nclass PositionalEmbedding(torch.nn.Module):\n    def __init__(self, d_model, max_len=SEQ_LEN):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model).float()\n        print(pe.shape)\n        pe.require_grad = False\n        position = torch.arange(0, max_len).float().unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n        print('div_term',div_term)\n        print('div_term_dim',div_term.shape)\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term) \n        print('pe s',pe.shape)\n        print('pe',pe)\n        self.register_buffer('pe', pe.unsqueeze(0))\n\n    def forward(self, x):\n        return self.pe[:, :x.size(1)]\n\nclass BERTEmbedding(torch.nn.Module):\n    def __init__(self, vocab_size, embed_size, seq_len=SEQ_LEN, dropout=0.1):\n        super().__init__()\n        self.token = torch.nn.Embedding(vocab_size, embed_size, padding_idx=0)\n        self.position = PositionalEmbedding(embed_size, seq_len)\n        self.dropout = torch.nn.Dropout(p=dropout)\n        self.embed_size = embed_size\n\n    def forward(self, sequence):\n        x = self.token(sequence) + self.position(sequence)\n        return self.dropout(x)\n\nclass MultiHeadedAttention(torch.nn.Module):\n    def __init__(self, heads, d_model, dropout=0.1):\n        super(MultiHeadedAttention, self).__init__()\n        assert d_model % heads == 0\n        self.d_k = d_model // heads\n        self.heads = heads\n        self.dropout = torch.nn.Dropout(dropout)\n        self.query = torch.nn.Linear(d_model, d_model)\n        self.key = torch.nn.Linear(d_model, d_model)\n        self.value = torch.nn.Linear(d_model, d_model)\n        self.output_linear = torch.nn.Linear(d_model, d_model)\n\n    def forward(self, query, key, value, mask):\n        batch_size = query.size(0)\n        query = self.query(query).view(batch_size, -1, self.heads, self.d_k).transpose(1, 2)\n        key = self.key(key).view(batch_size, -1, self.heads, self.d_k).transpose(1, 2)\n        value = self.value(value).view(batch_size, -1, self.heads, self.d_k).transpose(1, 2)\n\n        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(query.size(-1))\n        scores = scores.masked_fill(mask == 0, -1e9)\n\n        weights = F.softmax(scores, dim=-1)\n        weights = self.dropout(weights)\n\n        context = torch.matmul(weights, value)\n        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.heads * self.d_k)\n        return self.output_linear(context)\n\nclass BERT(torch.nn.Module):\n    def __init__(self, vocab_size=VOCAB_SIZE, d_model=EMBED_LEN, n_layers=12, heads=12, dropout=0.1):\n        super().__init__()\n        self.embedding = BERTEmbedding(vocab_size, d_model, SEQ_LEN)\n        self.encoder_layers = torch.nn.ModuleList([EncoderLayer(d_model, heads, d_model * 4, dropout) for _ in range(n_layers)])\n\n    def forward(self, x, attention_mask):\n        x = self.embedding(x)\n        mask = attention_mask.unsqueeze(1).unsqueeze(2)\n        for layer in self.encoder_layers:\n            x = layer(x, mask)\n        return x\n        \nclass ClassificationModel(torch.nn.Module):\n    def __init__(self, hidden):\n        super().__init__()\n        # Create separate linear layers for each classification task\n        self.linear1 = torch.nn.Linear(hidden, 2)\n        self.linear2 = torch.nn.Linear(hidden, 2)\n        self.softmax = torch.nn.LogSoftmax(dim=-1)\n    \n    def forward(self, x,task):\n        # Extract tokens\n        if(task==1):\n            token=x[:, 0, :]\n            return self.linear1(token)\n        else:\n            token=x[:, 1, :]\n            return self.linear2(token)\n        # Apply linear layers and find the class indices directly\n        # class_indices = torch.stack([\n        #     torch.argmax(self.linear1(token1), dim=-1),  # Shape: (batch_size,)\n        #     torch.argmax(self.linear2(token2), dim=-1),  # Shape: (batch_size,)\n        #     torch.argmax(self.linear3(token3), dim=-1),  # Shape: (batch_size,)\n        #     torch.argmax(self.linear4(token4), dim=-1)   # Shape: (batch_size,)\n        # ])  # Shape: (4, batch_size)\n\n        # Transpose to get the desired shape (batch_size, 4)\n        # class_indices = class_indices.transpose(0, 1)  # Shape: (batch_size, 4)\n\n        # return class_indices\n        \nclass BERTLM(torch.nn.Module):\n    def __init__(self, bert: BERT, vocab_size):\n        super().__init__()\n        self.bert = bert\n        self.class_lm=ClassificationModel(self.bert.embedding.embed_size)\n\n    def forward(self, x, attention_mask,task):\n        x = self.bert(x, attention_mask)\n        return self.class_lm(x,task)\n\nclass FeedForward(torch.nn.Module):\n    def __init__(self, d_model, middle_dim=2048, dropout=0.1):\n        super(FeedForward, self).__init__()\n        self.fc1 = torch.nn.Linear(d_model, middle_dim)\n        self.fc2 = torch.nn.Linear(middle_dim, d_model)\n        self.dropout = torch.nn.Dropout(dropout)\n        self.activation = torch.nn.GELU()\n\n    def forward(self, x):\n        out = self.activation(self.fc1(x))\n        out = self.fc2(self.dropout(out))\n        return out\n\nclass EncoderLayer(torch.nn.Module):\n    def __init__(self, d_model=EMBED_LEN, heads=12, feed_forward_hidden=EMBED_LEN*4, dropout=0.1):\n        super(EncoderLayer, self).__init__()\n        self.layernorm = torch.nn.LayerNorm(d_model)\n        self.self_multihead = MultiHeadedAttention(heads, d_model)\n        self.feed_forward = FeedForward(d_model, middle_dim=feed_forward_hidden)\n        self.dropout = torch.nn.Dropout(dropout)\n\n    def forward(self, embeddings, mask):\n        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n        interacted = self.layernorm(interacted + embeddings)\n        feed_forward_out = self.dropout(self.feed_forward(interacted))\n        encoded = self.layernorm(feed_forward_out + interacted)\n        return encoded\n\ndef train_bert(model, train_dataloader, optimizer,scheduler, device, epoch_start,epoch_end):\n    model.train()\n    for epoch in range(epoch_start,epoch_end):\n        total_loss = 0\n        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\"):\n            optimizer.zero_grad()\n            bert_input = batch['bert_input'].to(device)\n            bert_label = batch['bert_label'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            bert_label=bert_label.transpose(0,1)\n            for i in range(2):\n                outputs = model(bert_input, attention_mask,i)\n                \n                # print('outputs',outputs)\n                # print('labels',bert_label)\n                \n                loss=F.cross_entropy(outputs,bert_label[i])\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                optimizer.step()\n                scheduler.step()\n                total_loss += loss.item()\n         \n        avg_loss = total_loss / len(train_dataloader)\n        print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss:.4f}\")\n\ndataset = BERTDataset(conv, tokenizer,labels)\ntrain_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n\nprint(f\"Vocabulary size: {tokenizer.get_vocab_size()}\")\nprint(f\"Number of sentences: {len(dataset)}\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\nbert = BERT(vocab_size=VOCAB_SIZE, d_model=EMBED_LEN, n_layers=12, heads=12)\nmodel = BERTLM(bert, vocab_size=VOCAB_SIZE)\nmodel.to(device)\noptimizer = AdamW(model.parameters(), lr=2e-5)\nWARMUP_STEPS=10000\nEPOCHS=20\ntotal_steps = len(train_loader) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=total_steps)","metadata":{"id":"XTthZzTranCU","execution":{"iopub.status.busy":"2024-11-19T04:00:51.359629Z","iopub.execute_input":"2024-11-19T04:00:51.360696Z","iopub.status.idle":"2024-11-19T04:00:51.578086Z","shell.execute_reply.started":"2024-11-19T04:00:51.360644Z","shell.execute_reply":"2024-11-19T04:00:51.577130Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Vocabulary size: 20000\nNumber of sentences: 6396\nUsing device: cuda\ntorch.Size([68, 252])\ndiv_term tensor([1.0000e+00, 9.2951e-01, 8.6399e-01, 8.0309e-01, 7.4648e-01, 6.9386e-01,\n        6.4495e-01, 5.9948e-01, 5.5723e-01, 5.1795e-01, 4.8144e-01, 4.4750e-01,\n        4.1596e-01, 3.8664e-01, 3.5938e-01, 3.3405e-01, 3.1050e-01, 2.8861e-01,\n        2.6827e-01, 2.4936e-01, 2.3178e-01, 2.1544e-01, 2.0026e-01, 1.8614e-01,\n        1.7302e-01, 1.6082e-01, 1.4949e-01, 1.3895e-01, 1.2915e-01, 1.2005e-01,\n        1.1159e-01, 1.0372e-01, 9.6411e-02, 8.9615e-02, 8.3298e-02, 7.7426e-02,\n        7.1969e-02, 6.6895e-02, 6.2180e-02, 5.7797e-02, 5.3723e-02, 4.9936e-02,\n        4.6416e-02, 4.3144e-02, 4.0103e-02, 3.7276e-02, 3.4648e-02, 3.2206e-02,\n        2.9936e-02, 2.7826e-02, 2.5864e-02, 2.4041e-02, 2.2346e-02, 2.0771e-02,\n        1.9307e-02, 1.7946e-02, 1.6681e-02, 1.5505e-02, 1.4412e-02, 1.3396e-02,\n        1.2452e-02, 1.1574e-02, 1.0758e-02, 1.0000e-02, 9.2951e-03, 8.6399e-03,\n        8.0309e-03, 7.4648e-03, 6.9386e-03, 6.4495e-03, 5.9948e-03, 5.5723e-03,\n        5.1795e-03, 4.8144e-03, 4.4750e-03, 4.1596e-03, 3.8664e-03, 3.5938e-03,\n        3.3405e-03, 3.1050e-03, 2.8861e-03, 2.6827e-03, 2.4936e-03, 2.3178e-03,\n        2.1544e-03, 2.0026e-03, 1.8614e-03, 1.7302e-03, 1.6082e-03, 1.4949e-03,\n        1.3895e-03, 1.2915e-03, 1.2005e-03, 1.1159e-03, 1.0372e-03, 9.6411e-04,\n        8.9615e-04, 8.3298e-04, 7.7426e-04, 7.1969e-04, 6.6896e-04, 6.2180e-04,\n        5.7797e-04, 5.3723e-04, 4.9936e-04, 4.6416e-04, 4.3144e-04, 4.0103e-04,\n        3.7276e-04, 3.4648e-04, 3.2206e-04, 2.9936e-04, 2.7826e-04, 2.5864e-04,\n        2.4041e-04, 2.2346e-04, 2.0771e-04, 1.9307e-04, 1.7946e-04, 1.6681e-04,\n        1.5505e-04, 1.4412e-04, 1.3396e-04, 1.2452e-04, 1.1574e-04, 1.0758e-04])\ndiv_term_dim torch.Size([126])\npe s torch.Size([68, 252])\npe tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n          0.0000e+00,  1.0000e+00],\n        [ 8.4147e-01,  5.4030e-01,  8.0133e-01,  ...,  1.0000e+00,\n          1.0758e-04,  1.0000e+00],\n        [ 9.0930e-01, -4.1615e-01,  9.5875e-01,  ...,  1.0000e+00,\n          2.1517e-04,  1.0000e+00],\n        ...,\n        [ 8.2683e-01, -5.6245e-01, -6.6529e-01,  ...,  9.9997e-01,\n          6.9929e-03,  9.9998e-01],\n        [-2.6551e-02, -9.9965e-01, -9.9625e-01,  ...,  9.9997e-01,\n          7.1005e-03,  9.9997e-01],\n        [-8.5552e-01, -5.1777e-01, -5.2669e-01,  ...,  9.9997e-01,\n          7.2080e-03,  9.9997e-01]])\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# tamil_text_files = ['/kaggle/input/wikidata/diamond_merged.txt','/kaggle/input/wikidata/main_train_valid.txt' ]\ntamil_text_files=['/kaggle/input/iconst/data.txt']\ntokenizer = create_tokenizer(tamil_text_files)  ","metadata":{"execution":{"iopub.status.busy":"2024-11-19T03:56:37.600390Z","iopub.execute_input":"2024-11-19T03:56:37.601045Z","iopub.status.idle":"2024-11-19T03:56:38.269370Z","shell.execute_reply.started":"2024-11-19T03:56:37.601010Z","shell.execute_reply":"2024-11-19T03:56:38.268380Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\n\n\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"train_bert(model,train_loader,optimizer,scheduler,device,0,10) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T04:00:56.130211Z","iopub.execute_input":"2024-11-19T04:00:56.130909Z","iopub.status.idle":"2024-11-19T04:04:20.315488Z","shell.execute_reply.started":"2024-11-19T04:00:56.130875Z","shell.execute_reply":"2024-11-19T04:04:20.314692Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 200/200 [00:20<00:00,  9.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Average Loss: 1.4436\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 200/200 [00:20<00:00,  9.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Average Loss: 1.3832\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 200/200 [00:20<00:00,  9.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Average Loss: 1.3734\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 200/200 [00:20<00:00,  9.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Average Loss: 1.3649\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 200/200 [00:20<00:00,  9.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Average Loss: 1.3630\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|██████████| 200/200 [00:20<00:00,  9.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6, Average Loss: 1.3573\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|██████████| 200/200 [00:20<00:00,  9.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7, Average Loss: 1.3531\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8: 100%|██████████| 200/200 [00:20<00:00,  9.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8, Average Loss: 1.3504\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9: 100%|██████████| 200/200 [00:20<00:00,  9.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9, Average Loss: 1.3510\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10: 100%|██████████| 200/200 [00:20<00:00,  9.83it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 10, Average Loss: 1.3513\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"raw","source":"   ","metadata":{"execution":{"iopub.status.busy":"2024-11-18T11:56:46.424110Z","iopub.execute_input":"2024-11-18T11:56:46.424800Z","iopub.status.idle":"2024-11-18T11:56:46.465673Z","shell.execute_reply.started":"2024-11-18T11:56:46.424763Z","shell.execute_reply":"2024-11-18T11:56:46.464532Z"}}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"def load_model(load_directory,model):\n    model_path = os.path.join(load_directory, \"bert_model_17_epoch.pth\")\n    print(f\"Model and vocabulary loaded from {load_directory}\")\n    model.load_state_dict(torch.load(model_path))\n    return model\nmodel=load_model(\"/kaggle/input/bert/pytorch/abc/1\", model)","metadata":{"execution":{"iopub.status.busy":"2024-09-24T16:34:22.902706Z","iopub.execute_input":"2024-09-24T16:34:22.903237Z","iopub.status.idle":"2024-09-24T16:34:23.941653Z","shell.execute_reply.started":"2024-09-24T16:34:22.903193Z","shell.execute_reply":"2024-09-24T16:34:23.940579Z"}}},{"cell_type":"code","source":"# def load_model(load_directory,model): \n#     model_path = os.path.join(load_directory, \"bert_model_24_epoch.pth\") \n#     print(f\"Model and vocabulary loaded from {load_directory}\") \n#     model.load_state_dict(torch.load(model_path)) \n#     return model \n# model=load_model(\"/kaggle/input/abcd/pytorch/default/1/\", model)","metadata":{"execution":{"iopub.status.busy":"2024-11-18T15:01:55.680042Z","iopub.execute_input":"2024-11-18T15:01:55.680431Z","iopub.status.idle":"2024-11-18T15:01:55.685424Z","shell.execute_reply.started":"2024-11-18T15:01:55.680393Z","shell.execute_reply":"2024-11-18T15:01:55.684355Z"},"trusted":true},"outputs":[],"execution_count":57},{"cell_type":"code","source":" ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.bert","metadata":{"execution":{"iopub.status.busy":"2024-09-25T06:22:35.559124Z","iopub.execute_input":"2024-09-25T06:22:35.559513Z","iopub.status.idle":"2024-09-25T06:22:35.567396Z","shell.execute_reply.started":"2024-09-25T06:22:35.559474Z","shell.execute_reply":"2024-09-25T06:22:35.566494Z"},"trusted":true},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"BERT(\n  (embedding): BERTEmbedding(\n    (token): Embedding(50000, 252, padding_idx=0)\n    (position): PositionalEmbedding()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder_layers): ModuleList(\n    (0-11): 12 x EncoderLayer(\n      (layernorm): LayerNorm((252,), eps=1e-05, elementwise_affine=True)\n      (self_multihead): MultiHeadedAttention(\n        (dropout): Dropout(p=0.1, inplace=False)\n        (query): Linear(in_features=252, out_features=252, bias=True)\n        (key): Linear(in_features=252, out_features=252, bias=True)\n        (value): Linear(in_features=252, out_features=252, bias=True)\n        (output_linear): Linear(in_features=252, out_features=252, bias=True)\n      )\n      (feed_forward): FeedForward(\n        (fc1): Linear(in_features=252, out_features=1008, bias=True)\n        (fc2): Linear(in_features=1008, out_features=252, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (activation): GELU(approximate='none')\n      )\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"model.bert.embedding.position.pe.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-23T07:55:56.855885Z","iopub.execute_input":"2024-09-23T07:55:56.856365Z","iopub.status.idle":"2024-09-23T07:55:56.864212Z","shell.execute_reply.started":"2024-09-23T07:55:56.856320Z","shell.execute_reply":"2024-09-23T07:55:56.862952Z"},"trusted":true},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 68, 252])"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"all_embedding=model.bert.embedding.token.weight\nvocab = tokenizer.get_vocab()","metadata":{"execution":{"iopub.status.busy":"2024-09-23T07:57:16.058911Z","iopub.execute_input":"2024-09-23T07:57:16.059748Z","iopub.status.idle":"2024-09-23T07:57:16.098129Z","shell.execute_reply.started":"2024-09-23T07:57:16.059705Z","shell.execute_reply":"2024-09-23T07:57:16.096770Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"extract embedding from bert","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\ndef get_token_embeddings_custom_bert(model, input_ids):\n    print(input_ids)\n    with torch.no_grad():\n        embeddings = model.bert.embedding(input_ids) \n        for layer in model.bert.encoder_layers:\n            embeddings = layer(embeddings)  \n    \n    return embeddings\n\ndef get_embedding(model, tokenizer, word, top_n=10):\n    dataset = BERTDataset([word], tokenizer)\n    data=dataset.__getitem__(0,flag=False)\n    input_ids = data['bert_input'].unsqueeze(0).to(device)\n    attention_mask = data['attention_mask'].unsqueeze(0).to(device)\n    masked_lm_labels = data['bert_label'].unsqueeze(0).to(device)  \n    \n    word_embeddings = get_token_embeddings_custom_bert(model, input_ids)\n    word_embedding = torch.mean(word_embeddings, dim=1).squeeze(0)\n\n    # Now compute cosine similarity between this word's embedding and all token embeddings\n    all_embeddings = model.bert.embedding.token.weight \n    cosine_similarities = F.cosine_similarity(word_embedding.unsqueeze(0), all_embeddings, dim=1)\n\n    # Get the top N most similar tokens (excluding the queried word itself)\n    top_indices = torch.argsort(cosine_similarities, descending=True)[1:top_n+1]\n    \n    # Convert the token indices back to words\n    similar_words = [tokenizer.convert_ids_to_tokens(idx.item()) for idx in top_indices]\n    \n    print(f\"Top {top_n} words similar to '{word}': {similar_words}\")\n\nword = \"குடியரசு\"\nget_embedding(model, tokenizer, word, top_n=10)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"find_similar_words('முதல',tokenizer,model)","metadata":{"execution":{"iopub.status.busy":"2024-09-25T08:19:23.132313Z","iopub.execute_input":"2024-09-25T08:19:23.132722Z","iopub.status.idle":"2024-09-25T08:19:23.141992Z","shell.execute_reply.started":"2024-09-25T08:19:23.132684Z","shell.execute_reply":"2024-09-25T08:19:23.141064Z"},"trusted":true},"outputs":[{"name":"stdout","text":"tensor([37540, 27365, 12787,  6861, 21004], device='cuda:0')\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"['செய்தக்க', 'இப்பாடசாலை', '##ளையின்', 'Al', 'நாயர்']"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"special_tokens = [\"[PAD]\", \"[MASK]\", \"[UNK]\"]\nvocab=tokenizer.get_vocab()\nlen(vocab)\nfor i in special_tokens:\n    print(vocab[i])","metadata":{"execution":{"iopub.status.busy":"2024-09-22T13:20:12.578516Z","iopub.execute_input":"2024-09-22T13:20:12.578947Z","iopub.status.idle":"2024-09-22T13:20:12.619497Z","shell.execute_reply.started":"2024-09-22T13:20:12.578905Z","shell.execute_reply":"2024-09-22T13:20:12.618415Z"},"trusted":true},"outputs":[{"name":"stdout","text":"0\n1\n2\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"key_int_value_word={}\nfor key,value in vocab.items():\n    key_int_value_word[value]=key","metadata":{"execution":{"iopub.status.busy":"2024-09-22T07:22:00.993288Z","iopub.execute_input":"2024-09-22T07:22:00.994095Z","iopub.status.idle":"2024-09-22T07:22:01.012981Z","shell.execute_reply.started":"2024-09-22T07:22:00.994054Z","shell.execute_reply":"2024-09-22T07:22:01.011948Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"check the unknown count in the built tensors for the sentence input","metadata":{}},{"cell_type":"code","source":"# count=0\n# for i in range(80000,85000):\n#     example = dataset[i]\n# #     print(f\"Example {i + 1}:\")\n#     a= example['bert_input']\n# #     print(\"BERT Label Tokens: \", example['bert_label'])\n# #     print(\"Attention Mask: \", example['attention_mask'])\n# # #     print() \n#     a=a.tolist()\n# #     l=[]\n#     for j in a:\n# #         l.append(key_int_value_word[j])\n#         if j==2:\n#             count+=1\n#         if j==0:\n#             break\n#     if i%1000==0:\n#             print(i)\n# #         print(j,end=' ')\n# #     print()\n# #     print(l)\n# print(count)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T13:24:22.136691Z","iopub.execute_input":"2024-09-22T13:24:22.137550Z","iopub.status.idle":"2024-09-22T13:24:47.260861Z","shell.execute_reply.started":"2024-09-22T13:24:22.137508Z","shell.execute_reply":"2024-09-22T13:24:47.259727Z"},"trusted":true},"outputs":[{"name":"stdout","text":"80000\n81000\n82000\n83000\n84000\n0\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"vocab=tokenizer.get_vocab()\n# sorted_vocab = dict(sorted(vocab.items(), key=lambda item: item[1]))\n\n# with open('output_wiki_test1.txt','w',encoding='utf-8') as f:\n#     for key,value in sorted_vocab.items():\n#         f.write(f'{key}\\t\\t{value} \\n')   ","metadata":{"execution":{"iopub.status.busy":"2024-09-25T06:28:38.851803Z","iopub.execute_input":"2024-09-25T06:28:38.852196Z","iopub.status.idle":"2024-09-25T06:28:38.885995Z","shell.execute_reply.started":"2024-09-25T06:28:38.852158Z","shell.execute_reply":"2024-09-25T06:28:38.884896Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef test_bert_model(model, tokenizer, sentence, device,target,top_n=5):\n    dataset = BERTDataset([sentence], tokenizer)\n    dataset_target=BERTDataset([target],tokenizer)\n    data_target=dataset_target.__getitem__(0,False)\n    data=dataset.__getitem__(0,False)\n#     print(data)\n#     print(data_target)\n    target_list_tokenized=[]\n    target_input_ids=data_target['bert_input'].unsqueeze(0).to(device)\n    target_list=target_input_ids.tolist()\n    \n    for i in target_list[0]:\n        if i==0:\n            break\n        else:\n            target_list_tokenized.append(i)\n    #print('target',target_list_tokenized)\n    input_ids = data['bert_input'].unsqueeze(0).to(device)\n    #print(input_ids)\n    attention_mask = data['attention_mask'].unsqueeze(0).to(device)\n    \n    key_indexes=[input_ids.tolist()[0].index(i) for i in target_list_tokenized ]\n    print(key_indexes)\n    \n    #print(input_ids.shape)\n    model.eval()\n    predictions=[]\n    with torch.no_grad():\n        outputs=model.bert(input_ids, attention_mask)\n        #print('out',outputs.shape)\n    \n    context_embedding_list=[]\n    output=outputs.tolist()\n    for i in key_indexes:\n        context_embedding_list.append(output[0][i])\n    context_embedding=[0]*252\n    for context in context_embedding_list:\n        context_embedding = [context_embedding[i] + context[i] for i in range(len(context))]\n   # print(len(context_embedding))\n    context_embedding_tensor = torch.tensor(context_embedding, dtype=torch.float32).to(device)\n    #print(context_embedding_tensor)\n    all_embeddings = model.bert.embedding.token.weight \n    cosine_similarities = F.cosine_similarity(context_embedding_tensor.unsqueeze(0), all_embeddings, dim=1)\n\n    # Get the top N most similar tokens (excluding the queried word itself)\n    top_indices = torch.argsort(cosine_similarities, descending=True)[1:10]\n    print('top',top_indices)\n    # Convert the token indices back to words\n    return [tokenizer.id_to_token(idx.item()) for idx in top_indices]\n    ","metadata":{"execution":{"iopub.status.busy":"2024-09-25T08:21:48.652046Z","iopub.execute_input":"2024-09-25T08:21:48.652935Z","iopub.status.idle":"2024-09-25T08:21:48.664935Z","shell.execute_reply.started":"2024-09-25T08:21:48.652893Z","shell.execute_reply":"2024-09-25T08:21:48.663887Z"},"trusted":true},"outputs":[],"execution_count":31},{"cell_type":"code","source":"test_sentence = \"அகர முதல எழுத்தெல்லாம் ஆதி பகவன் முதற்றே உலகு\"\nprint(test_bert_model(model, tokenizer, test_sentence, device,\"அகர\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\ndef predict_masked_words(model, tokenizer, sentence, device):\n    model.eval()\n    tokens = tokenizer.encode(sentence).ids\n    print('tokens',tokens)\n    mask_token_id = tokenizer.token_to_id('[MASK]')\n    print('mask token id',mask_token_id)\n    mask_positions = [i for i, token in enumerate(tokens) if token == mask_token_id]\n    print('mask_position',mask_positions)\n    input_ids = torch.tensor([tokens]).to(device)\n    print('input ids',input_ids)\n    attention_mask = torch.ones_like(input_ids).to(device)\n    print('att',attention_mask)\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask)\n    print('outputs',outputs.shape)\n    predictions = []\n    for pos in range(10):\n        predicted_id = outputs[0, pos].argmax().item()\n        predicted_token = tokenizer.id_to_token(predicted_id)\n        predictions.append((pos, predicted_token))\n    \n    return predictions\n\n# def analyze_training_performance(losses, epochs):\n#     import matplotlib.pyplot as plt\n    \n#     plt.figure(figsize=(10, 6))\n#     plt.plot(range(1, epochs+1), losses)\n#     plt.title('Training Loss over Epochs')\n#     plt.xlabel('Epoch')\n#     plt.ylabel('Loss')\n#     plt.grid(True)\n#     plt.show()\n    \n#     print(f\"Initial loss: {losses[0]:.4f}\")\n#     print(f\"Final loss: {losses[-1]:.4f}\")\n#     print(f\"Absolute improvement: {losses[0] - losses[-1]:.4f}\")\n#     print(f\"Relative improvement: {(losses[0] - losses[-1]) / losses[0] * 100:.2f}%\")\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\ntest_sentence = \"அகர [MASK] எழுத்தெல்லாம் ஆதி [MASK] முதற்றே உலகு\"\npredictions = predict_masked_words(model, tokenizer, test_sentence, device)\n\nprint(\"Original sentence:\", test_sentence)\nprint(\"Predictions:\")\nfor pos, pred in predictions:\n    print(f\"Position {pos}: {pred}\")\n\n# # Assuming you've collected losses during training\n# training_losses = [...]  # List of loss values from each epoch\n# analyze_training_performance(training_losses, len(training_losses))","metadata":{"execution":{"iopub.status.busy":"2024-09-23T04:19:04.126973Z","iopub.execute_input":"2024-09-23T04:19:04.127381Z","iopub.status.idle":"2024-09-23T04:19:04.157404Z","shell.execute_reply.started":"2024-09-23T04:19:04.127341Z","shell.execute_reply":"2024-09-23T04:19:04.156509Z"},"trusted":true},"outputs":[{"name":"stdout","text":"tokens [12644, 1, 3780, 4426, 5901, 1, 14422, 1283, 1271, 4958]\nmask token id 1\nmask_position [1, 5]\ninput ids tensor([[12644,     1,  3780,  4426,  5901,     1, 14422,  1283,  1271,  4958]],\n       device='cuda:0')\natt tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\noutputs torch.Size([1, 10, 50000])\nOriginal sentence: அகர [MASK] எழுத்தெல்லாம் ஆதி [MASK] முதற்றே உலகு\nPredictions:\nPosition 0: அகர\nPosition 1: பொருள்\nPosition 2: எழுத்த\nPosition 3: ##ெல்லாம்\nPosition 4: ஆதி\nPosition 5: என்று\nPosition 6: முதற்\nPosition 7: ##ற\nPosition 8: ##ே\nPosition 9: உலகு\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"# torch.cuda.empty_cache()","metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# save_directory = \"../models/diamond_merged_tokenized\"\n# save_model(model, tokenizer, save_directory)","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Model and vocabulary saved to ../models/valid_d_merged\n"}],"execution_count":21},{"cell_type":"code","source":"\ndef load_model(load_directory,model):\n    model_path = os.path.join(load_directory, \"bert_model11th_epoch.pth\")\n#     bert = BERT(vocab_size=vocab_size, d_model=d_model, n_layers=n_layers, heads=heads)\n#     model = BERTLM(bert, vocab_size=vocab_size)\n    print(f\"Model and vocabulary loaded from {load_directory}\")\n    model.load_state_dict(torch.load(model_path))\n    return model\n#     vocab_path = os.path.join(load_directory, \"vocab.json\")\n#     with open(vocab_path, 'r', encoding='utf-8') as f:\n#         vocab = json.load(f)\n    \n#     tokenizer = Tokenizer(models.WordLevel(vocab=vocab, unk_token=\"[UNK]\"))\n#     tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n    \n#     special_tokens = [\"[PAD]\",\"[MASK]\", \"[UNK]\"]\n#     tokenizer.add_special_tokens(special_tokens)\n\n#     tokenizer.post_processor = processors.TemplateProcessing(\n#         single=\"[CLS] $A [SEP]\",\n#         pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n#         special_tokens=[\n#             (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n#             (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n#         ],\n#     )\n    \n   \n#     return model, tokenizer\nmodel=load_model(\"/kaggle/input/bert/pytorch/default/1\", model)\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# loaded_model.to(device)\n\n# sentence = \"கற்றதனால் ஆய எல்லா பயன் என்கொல் நூல்களையும் கற்றவர்க்கு அக்கல்வி அறிவினாலாய பயன் யாது\"\n# word = \"என்கொல்\"\n# similar_words = find_contextual_similar_words(sentence, word, tokenizer, model, device)\n# print(f\"Words similar to '{word}' in the context of '{sentence}': {similar_words}\")\n\n# masked_sentence = \"கற்றதனால் [MASK] பயன் என்கொல் \"\n# predicted_word, predicted_sentence = predict_masked_word(masked_sentence, tokenizer, model, device)\n# print(f\"Original masked sentence: {masked_sentence}\")\n# print(f\"Predicted word: {predicted_word}\")\n# print(f\"Predicted sentence: {predicted_sentence}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-23T08:27:59.275711Z","iopub.execute_input":"2024-09-23T08:27:59.276133Z","iopub.status.idle":"2024-09-23T08:27:59.473923Z","shell.execute_reply.started":"2024-09-23T08:27:59.276092Z","shell.execute_reply":"2024-09-23T08:27:59.472965Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Model and vocabulary loaded from /kaggle/input/bert/pytorch/default/1\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/86503471.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path))\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"\ndef find_contextual_similar_words(sentence, word, tokenizer, model, device, top_n=5):\n    model.eval()\n    with torch.no_grad():\n        tokens = [tokenizer.token_to_id(token) for token in sentence.split()]\n        word_id = tokenizer.token_to_id(word)\n        if word_id is None or word_id not in tokens:\n            return []\n        word_position = tokens.index(word_id)\n        print(tokens)\n        input_ids = torch.tensor([tokens]).to(device)\n        attention_mask = torch.ones_like(input_ids).to(device)\n        outputs = model.bert(input_ids, attention_mask)\n        contextual_embedding = outputs[0, word_position]\n        all_embeddings = model.bert.embedding.token.weight\n        cosine_similarities = F.cosine_similarity(contextual_embedding, all_embeddings)\n        top_indices = torch.argsort(cosine_similarities, descending=True)[1:top_n+1]\n        similar_words = [tokenizer.id_to_token(idx.item()) for idx in top_indices]\n        \n        return similar_words\n\nsentence = \"வானூர்தி நிலையங்கள் உள்ளன\"\nword = \"வானூர்தி\"\nsimilar_words = find_contextual_similar_words(sentence, word, tokenizer, model, device)\nprint(f\"Words similar to '{word}' in the context of '{sentence}': {similar_words}\") ","metadata":{"id":"tvuR898Z8ZkK","execution":{"iopub.status.busy":"2024-09-23T03:58:25.401842Z","iopub.execute_input":"2024-09-23T03:58:25.402573Z","iopub.status.idle":"2024-09-23T03:58:25.842087Z","shell.execute_reply.started":"2024-09-23T03:58:25.402530Z","shell.execute_reply":"2024-09-23T03:58:25.841115Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[7408, 11397, 2769]\nWords similar to 'வானூர்தி' in the context of 'வானூர்தி நிலையங்கள் உள்ளன': ['1995', 'rep', 'தேவைப்படுகின்றன', '##ோயிக்', 'ஒன்றனைச்']\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"***claude code***","metadata":{"id":"fr4T99aD8XwC"}},{"cell_type":"code","source":"","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"duDDjVH2_3Dp","outputId":"4917c691-489b-4b7b-e2b1-837f5e68f114","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with torch.no_grad():\n        word_id = tokenizer.token_to_id(\"விரிநீர்\")\n        if word_id is None:\n            print('None')\n        word_embedding = model.bert.embedding.token.weight[word_id]\n        cosine_similarities = F.cosine_similarity(word_embedding, model.bert.embedding.token.weight)\n        top_indices = torch.argsort(cosine_similarities, descending=True)[1:5]\n        print([tokenizer.id_to_token(idx.item()) for idx in top_indices])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xZnSzTyREp2X","outputId":"c3b59dba-ef85-4f7f-f79e-73a155ca3778"},"outputs":[{"name":"stdout","output_type":"stream","text":"['நன்றல்லது', 'உயிருடையார்', 'ஒழுக்கத்தை', 'சாலும்']\n"}],"execution_count":100},{"cell_type":"code","source":"# def print_vocabulary(tokenizer):\n#     vocab = tokenizer.get_vocab()\n#     sorted_vocab=sorted(vocab.items(), key=lambda x: x[1])\n#     print(\"Vocabulary:\")\n#     for word, index in sorted_vocab:\n#         print(word,end=' ')\n#     print(f\"Total vocabulary size: {len(vocab)}\")\n","metadata":{"id":"ZWbvmAh2E6H_"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}
{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9448963,"sourceType":"datasetVersion","datasetId":5743086},{"sourceId":9941909,"sourceType":"datasetVersion","datasetId":6112794},{"sourceId":118881,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":99966,"modelId":124137},{"sourceId":119356,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":100370,"modelId":124137},{"sourceId":120158,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":101071,"modelId":125250}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport torch\nimport re\nimport random\nimport torch\nfrom torch.utils.data import Dataset\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordPiece\nfrom tokenizers.pre_tokenizers import Whitespace\nfrom tokenizers.trainers import WordPieceTrainer\nimport random\nfrom tokenizers import BertWordPieceTokenizer\nfrom transformers import BertTokenizer\nimport tqdm\nfrom torch.utils.data import Dataset, DataLoader\nimport itertools\nimport math\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch.optim import Adam","metadata":{"id":"Z_p8twHVZDLx","trusted":true,"execution":{"iopub.status.busy":"2024-11-19T09:49:47.395232Z","iopub.execute_input":"2024-11-19T09:49:47.395600Z","iopub.status.idle":"2024-11-19T09:49:47.401105Z","shell.execute_reply.started":"2024-11-19T09:49:47.395572Z","shell.execute_reply":"2024-11-19T09:49:47.400237Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"merge= r'/kaggle/input/wikidata/diamond_merged.txt' # Parimelagar urai with its explanation and  Random Merge 20 times\n# train = r'../datasets/main_train_train.txt'\ntest = r'/kaggle/input/wikidata/main_train_valid.txt' # total 400MB\niconst=r'/kaggle/input/iconst/data.txt'","metadata":{"id":"_-7ywb1aZcNn","execution":{"iopub.status.busy":"2024-11-19T09:49:48.286286Z","iopub.execute_input":"2024-11-19T09:49:48.286603Z","iopub.status.idle":"2024-11-19T09:49:48.290643Z","shell.execute_reply.started":"2024-11-19T09:49:48.286579Z","shell.execute_reply":"2024-11-19T09:49:48.289861Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"conv=[]\n# with open( merge, 'r',encoding='utf-8') as c:\n#     merged = c.readlines()           \n# # with open( train, 'r',encoding='utf-8') as c:\n# #     trained = c.readlines()\n# with open( test, 'r',encoding='utf-8') as c:\n#     tested = c.readlines()\nwith open( iconst, 'r',encoding='utf-8') as c:\n    icst = c.readlines()  \nconv=icst","metadata":{"id":"qcAna19JZtnb","execution":{"iopub.status.busy":"2024-11-19T09:49:48.966463Z","iopub.execute_input":"2024-11-19T09:49:48.967199Z","iopub.status.idle":"2024-11-19T09:49:48.999112Z","shell.execute_reply.started":"2024-11-19T09:49:48.967164Z","shell.execute_reply":"2024-11-19T09:49:48.998435Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# conv=tested+merged\n","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:49:49.397897Z","iopub.execute_input":"2024-11-19T09:49:49.398767Z","iopub.status.idle":"2024-11-19T09:49:49.402266Z","shell.execute_reply.started":"2024-11-19T09:49:49.398732Z","shell.execute_reply":"2024-11-19T09:49:49.401396Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"len(conv)#,len(trained),len(tested) Total  sentences","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:49:50.021326Z","iopub.execute_input":"2024-11-19T09:49:50.021965Z","iopub.status.idle":"2024-11-19T09:49:50.028042Z","shell.execute_reply.started":"2024-11-19T09:49:50.021933Z","shell.execute_reply":"2024-11-19T09:49:50.027078Z"},"trusted":true},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"6396"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"sum=0\nl=0\ndic={}\nword_freq_count={}\ns=set()\nfor sent in conv:\n    length=len(sent.split())\n    l=max(l,length)\n    for word in sent.split():\n        if word in word_freq_count.keys():\n            word_freq_count[word]=word_freq_count[word]+1\n        else:\n            word_freq_count[word]=1\n        s.add(word)\n    dic[length]=1 if length not in dic.keys() else dic[length]+1\nlen(s),l                                                           ","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:49:50.064620Z","iopub.execute_input":"2024-11-19T09:49:50.064859Z","iopub.status.idle":"2024-11-19T09:49:50.163782Z","shell.execute_reply.started":"2024-11-19T09:49:50.064836Z","shell.execute_reply":"2024-11-19T09:49:50.162916Z"},"trusted":true},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(22324, 233)"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"word_freq_count_above_10={}\nfor key,value in word_freq_count.items():\n    if value>10:\n        word_freq_count_above_10[key]=value\nprint(len(word_freq_count_above_10))","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:49:50.959817Z","iopub.execute_input":"2024-11-19T09:49:50.960186Z","iopub.status.idle":"2024-11-19T09:49:50.967941Z","shell.execute_reply.started":"2024-11-19T09:49:50.960153Z","shell.execute_reply":"2024-11-19T09:49:50.966983Z"},"trusted":true},"outputs":[{"name":"stdout","text":"1769\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"sorted_dict = dict(sorted(dic.items()))\nsorted_dict    #sentence Length","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:49:51.254938Z","iopub.execute_input":"2024-11-19T09:49:51.255303Z","iopub.status.idle":"2024-11-19T09:49:51.262637Z","shell.execute_reply.started":"2024-11-19T09:49:51.255274Z","shell.execute_reply":"2024-11-19T09:49:51.261795Z"},"trusted":true},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"{0: 2,\n 1: 24,\n 2: 72,\n 3: 93,\n 4: 109,\n 5: 122,\n 6: 128,\n 7: 142,\n 8: 142,\n 9: 141,\n 10: 129,\n 11: 129,\n 12: 135,\n 13: 141,\n 14: 126,\n 15: 141,\n 16: 130,\n 17: 136,\n 18: 151,\n 19: 152,\n 20: 148,\n 21: 109,\n 22: 142,\n 23: 128,\n 24: 134,\n 25: 122,\n 26: 118,\n 27: 133,\n 28: 126,\n 29: 84,\n 30: 112,\n 31: 116,\n 32: 102,\n 33: 94,\n 34: 122,\n 35: 98,\n 36: 105,\n 37: 93,\n 38: 120,\n 39: 106,\n 40: 122,\n 41: 112,\n 42: 150,\n 43: 164,\n 44: 145,\n 45: 174,\n 46: 161,\n 47: 140,\n 48: 105,\n 49: 93,\n 50: 95,\n 51: 55,\n 52: 41,\n 53: 36,\n 54: 27,\n 55: 14,\n 56: 12,\n 57: 12,\n 58: 7,\n 59: 7,\n 60: 5,\n 61: 3,\n 62: 3,\n 63: 1,\n 65: 3,\n 66: 1,\n 67: 2,\n 69: 3,\n 70: 2,\n 71: 4,\n 72: 3,\n 73: 1,\n 75: 2,\n 77: 1,\n 78: 1,\n 80: 1,\n 81: 1,\n 82: 2,\n 83: 1,\n 85: 1,\n 87: 2,\n 90: 1,\n 92: 1,\n 94: 1,\n 96: 1,\n 97: 1,\n 99: 2,\n 104: 1,\n 105: 1,\n 107: 1,\n 109: 1,\n 114: 1,\n 116: 1,\n 118: 1,\n 122: 1,\n 131: 1,\n 132: 1,\n 151: 1,\n 156: 1,\n 159: 1,\n 161: 1,\n 165: 1,\n 180: 1,\n 233: 1}"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"process labels","metadata":{}},{"cell_type":"code","source":"import json\n\nwith open(r'/kaggle/input/iconst/labels.json', 'r') as f:\n    data_v = json.load(f)\n\nlabels = [data_v[key] for key in sorted(data_v.keys())]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T09:49:52.319478Z","iopub.execute_input":"2024-11-19T09:49:52.320329Z","iopub.status.idle":"2024-11-19T09:49:52.343492Z","shell.execute_reply.started":"2024-11-19T09:49:52.320292Z","shell.execute_reply":"2024-11-19T09:49:52.342778Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels[:10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T09:49:53.184871Z","iopub.execute_input":"2024-11-19T09:49:53.185578Z","iopub.status.idle":"2024-11-19T09:49:53.191333Z","shell.execute_reply.started":"2024-11-19T09:49:53.185539Z","shell.execute_reply":"2024-11-19T09:49:53.190512Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"[['0', '0', '0', '0'],\n ['1', '1', '2', '3'],\n ['1', '0', '1', '1'],\n ['1', '1', '2', '1'],\n ['1', '0', '1', '1'],\n ['0', '0', '0', '0'],\n ['0', '0', '0', '0'],\n ['1', '1', '2', '1'],\n ['0', '0', '0', '0'],\n ['1', '0', '2', '2']]"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# import string\n# def remove_punctuation(sentences):\n#     translator = str.maketrans('', '', string.punctuation)\n#     cleaned_sentences = []\n#     for sentence in sentences:\n#         words = sentence.translate(translator).split()\n#         cleaned_sentence = \" \".join(words)\n#         cleaned_sentences.append(cleaned_sentence)\n    \n#     return cleaned_sentences","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:49:53.703813Z","iopub.execute_input":"2024-11-19T09:49:53.704186Z","iopub.status.idle":"2024-11-19T09:49:53.708147Z","shell.execute_reply.started":"2024-11-19T09:49:53.704154Z","shell.execute_reply":"2024-11-19T09:49:53.707166Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# # # conv=trained\n# train_cleaned=remove_punctuation(trained)\n# test_cleaned=remove_punctuation(tested)","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:49:54.221518Z","iopub.execute_input":"2024-11-19T09:49:54.221860Z","iopub.status.idle":"2024-11-19T09:49:54.225958Z","shell.execute_reply.started":"2024-11-19T09:49:54.221829Z","shell.execute_reply":"2024-11-19T09:49:54.225145Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# with open('main_train_train.txt','a',encoding='utf-8') as f:\n#     for line in train_cleaned:\n#         if len(line.split())<=64: \n#             f.writelines(f'{line}\\n')\n# with open('main_train_valid.txt','a',encoding='utf-8') as f:\n#     for line in test_cleaned:\n#         if len(line.split())<=64: \n#             f.writelines(f'{line}\\n')\n    ","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:49:55.272392Z","iopub.execute_input":"2024-11-19T09:49:55.272733Z","iopub.status.idle":"2024-11-19T09:49:55.276600Z","shell.execute_reply.started":"2024-11-19T09:49:55.272704Z","shell.execute_reply":"2024-11-19T09:49:55.275769Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"conv[:5]","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:49:55.649896Z","iopub.execute_input":"2024-11-19T09:49:55.650262Z","iopub.status.idle":"2024-11-19T09:49:55.655670Z","shell.execute_reply.started":"2024-11-19T09:49:55.650231Z","shell.execute_reply":"2024-11-19T09:49:55.654823Z"},"trusted":true},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"['USER Abhi tak 2000 ke note me mujhe GPS nano chip nahin mila\\n',\n 'USER USER Abe katiye tumse kuch huaa toh jata nahi bas bakri jaise maymay karte rehte ho behan ke lodo tumhari Tablighi jamaat ke karan corona faila aur Bhoomi Poojan mein sab ko screen aur full medical jach ke saath huaa bhosdiwalo\\n',\n 'USER Ye sab sazish hai bina saman ke koi kaise apne gher ja sakta hai dekh lena inn logo ke beech bahut aise jamaat ke wo corona honge jo delhi se nikle they\\n',\n 'abe jao tum to dasko pahle hi fash gye the jab tere dada ne talwar ke nokh par salwar pahena tha tera daram kabhi hindu kabhi muslim\\n',\n 'Ab ye afbah kaun faila Raha hai ki Shahhen bag ke Biryani me Population control ki Dawa Milli Hui hai\\n']"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# conv=trained\n# print(len(conv),len(trained))","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:49:55.657120Z","iopub.execute_input":"2024-11-19T09:49:55.657425Z","iopub.status.idle":"2024-11-19T09:49:55.666397Z","shell.execute_reply.started":"2024-11-19T09:49:55.657396Z","shell.execute_reply":"2024-11-19T09:49:55.665616Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# paths=['/kaggle/input/wikidata/diamond_merged.txt','/kaggle/input/wikidata/main_train_valid.txt' ]\npaths=['/kaggle/input/iconst/data.txt']","metadata":{"id":"s-l8YYCfc3pp","execution":{"iopub.status.busy":"2024-11-19T09:49:56.287866Z","iopub.execute_input":"2024-11-19T09:49:56.288491Z","iopub.status.idle":"2024-11-19T09:49:56.292438Z","shell.execute_reply.started":"2024-11-19T09:49:56.288455Z","shell.execute_reply":"2024-11-19T09:49:56.291479Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom tokenizers import Tokenizer, models, pre_tokenizers, processors\nimport random\nimport math\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam\nfrom tqdm import tqdm\nfrom tokenizers import trainers\nfrom tokenizers import Tokenizer, models, trainers, pre_tokenizers, processors\nimport torch\nfrom torch.utils.data import Dataset\nimport random\nimport json\nfrom torch.optim import AdamW\nfrom transformers import get_linear_schedule_with_warmup\n\nSEQ_LEN = 68\nEMBED_LEN = 288\nVOCAB_SIZE = 30000\n\ndef create_tokenizer(paths, vocab_size=VOCAB_SIZE):\n    tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n    tokenizer.pre_tokenizer = Whitespace()\n    special_tokens = [\"[CLS1]\",\"[CLS2]\",\"[PAD]\", \"[MASK]\", \"[UNK]\"]\n    tokenizer.add_special_tokens(special_tokens)\n\n    def batch_iterator(file_paths, batch_size=64):\n        for path in file_paths:\n            with open(path, 'r', encoding='utf-8') as f:\n                for line in f:\n                    yield line.strip()\n\n    trainer = WordPieceTrainer(\n        vocab_size=vocab_size,\n        special_tokens=special_tokens\n    )\n    tokenizer.train_from_iterator(batch_iterator(paths), trainer=trainer)\n    \n    return tokenizer\n\nclass BERTDataset(Dataset):\n    def __init__(self, sentences, tokenizer, labels,seq_len=SEQ_LEN):\n        self.tokenizer = tokenizer\n        self.seq_len = seq_len\n        self.sentences = sentences\n        self.labels=labels\n\n    def __len__(self):\n        return len(self.sentences)\n\n    def __getitem__(self, item,flag=False):\n        sentence = self.sentences[item]\n        encoding = self.tokenizer.encode(sentence)\n        # List of special class tokens\n        classes = [\n            self.tokenizer.token_to_id('[CLS1]'),\n            self.tokenizer.token_to_id('[CLS2]'),\n        ]\n        tokens =  classes+ encoding.ids  \n        if len(tokens) > self.seq_len:\n            tokens = tokens[:self.seq_len]\n        \n        padding_length = self.seq_len - len(tokens)\n        tokens = tokens + [self.tokenizer.token_to_id('[PAD]')] * padding_length\n        \n        mask = [1 if token != self.tokenizer.token_to_id('[PAD]') else 0 for token in tokens]\n        \n        bert_input = tokens.copy()\n        bert_label = [int(self.labels[item][0]), int(self.labels[item][1]), int(self.labels[item][2]), int(self.labels[item][3])]\n    \n        if flag:\n            for i in range(len(tokens)):\n                if mask[i] == 1 and random.random() < 0.25:\n                    if random.random() < 0.8:\n                        bert_input[i] = self.tokenizer.token_to_id('[MASK]')\n                    elif random.random() < 0.5:\n                        bert_input[i] = random.randint(0, self.tokenizer.get_vocab_size() - 1)\n        \n        return {\n            'bert_input': torch.tensor(bert_input),\n            'bert_label': torch.tensor(bert_label),\n            'attention_mask': torch.tensor(mask)\n        }\n\n    \nclass PositionalEmbedding(torch.nn.Module):\n    def __init__(self, d_model, max_len=SEQ_LEN):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model).float()\n        print(pe.shape)\n        pe.require_grad = False\n        position = torch.arange(0, max_len).float().unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n        print('div_term',div_term)\n        print('div_term_dim',div_term.shape)\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term) \n        print('pe s',pe.shape)\n        print('pe',pe)\n        self.register_buffer('pe', pe.unsqueeze(0))\n\n    def forward(self, x):\n        return self.pe[:, :x.size(1)]\n\nclass BERTEmbedding(torch.nn.Module):\n    def __init__(self, vocab_size, embed_size, seq_len=SEQ_LEN, dropout=0.1):\n        super().__init__()\n        self.token = torch.nn.Embedding(vocab_size, embed_size, padding_idx=0)\n        self.position = PositionalEmbedding(embed_size, seq_len)\n        self.dropout = torch.nn.Dropout(p=dropout)\n        self.embed_size = embed_size\n\n    def forward(self, sequence):\n        x = self.token(sequence) + self.position(sequence)\n        return self.dropout(x)\n\nclass MultiHeadedAttention(torch.nn.Module):\n    def __init__(self, heads, d_model, dropout=0.1):\n        super(MultiHeadedAttention, self).__init__()\n        \n        self.d_k = d_model // heads\n        self.heads = heads\n        self.dropout = nn.Dropout(dropout)\n        self.query = nn.Linear(d_model, d_model)\n        self.key = nn.Linear(d_model, d_model)\n        self.value = nn.Linear(d_model, d_model)\n        self.output_linear = nn.Linear(d_model, d_model)\n\n    def forward(self, query, key, value, mask=None):\n        batch_size = query.size(0)\n        query = self.query(query).view(batch_size, -1, self.heads, self.d_k).transpose(1, 2)\n        key = self.key(key).view(batch_size, -1, self.heads, self.d_k).transpose(1, 2)\n        value = self.value(value).view(batch_size, -1, self.heads, self.d_k).transpose(1, 2)\n        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.d_k)\n        scores = scores.masked_fill(mask == 0, -1e9)\n        weights = F.softmax(scores, dim=-1)\n        weights = self.dropout(weights)\n        context = torch.matmul(weights, value)\n        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.heads * self.d_k)\n        return self.output_linear(context)\n\nclass BERT(torch.nn.Module):\n    def __init__(self, vocab_size=VOCAB_SIZE, d_model=EMBED_LEN, n_layers=12, heads=12, dropout=0.1):\n        super().__init__()\n        self.embedding = BERTEmbedding(vocab_size, d_model, SEQ_LEN)\n        \n        self.encoder_layers = torch.nn.ModuleList([EncoderLayer(d_model, heads, d_model * 4, dropout) for _ in range(n_layers)])\n\n    def forward(self, x, attention_mask):\n        x = self.embedding(x)\n        mask = attention_mask.unsqueeze(1).unsqueeze(2)\n        for layer in self.encoder_layers:\n            x = layer(x, mask)\n        return x\n\nclass NaiveBayesClassifierWithBERT(nn.Module):\n    def __init__(self, hidden, num_classes, l1_lambda=0.0, l2_lambda=1e-5):\n        super().__init__()\n        self.hidden = hidden\n        self.num_classes = num_classes\n        self.l1_lambda = l1_lambda  # L1 regularization strength\n        self.l2_lambda = l2_lambda  # L2 regularization strength\n        self.class_priors = nn.Parameter(torch.zeros(num_classes))  # Prior distribution for classes\n        self.token_likelihoods = nn.Linear(hidden, num_classes, bias=False)  # Likelihood for tokens\n\n    def forward(self, x):\n        # Get the token contributions for each class\n        token_contributions = self.token_likelihoods(x)  \n        \n        token_contributions = token_contributions.sum(dim=1)  \n        log_probs = token_contributions + self.class_priors \n    \n        \n        return log_probs\n\n    def regularization_loss(self):\n        l2_loss = 0.0\n        for param in self.parameters():\n            l2_loss += torch.sum(param**2)\n        l1_loss = 0.0\n        for param in self.parameters():\n            l1_loss += torch.sum(torch.abs(param))\n\n        return self.l2_lambda * l2_loss + self.l1_lambda * l1_loss\n\nclass ClassificationModel(nn.Module):\n    def __init__(self, hidden, dropout_rate=0.5):\n        super().__init__()\n\n        # Task 1 (first branch)\n        self.linear1 = nn.Linear(hidden, 2*hidden)\n        self.bn1 = nn.BatchNorm1d(2*hidden)\n        self.dropout1 = nn.Dropout(dropout_rate)\n        \n        self.linear12 = nn.Linear(2*hidden, hidden)\n        self.bn12 = nn.BatchNorm1d(hidden)\n        self.dropout12 = nn.Dropout(dropout_rate)\n        \n        self.linear123 = nn.Linear(hidden, hidden // 2)\n        self.bn123 = nn.BatchNorm1d(hidden // 2)\n        self.dropout123 = nn.Dropout(dropout_rate)\n        \n        self.linear1234 = nn.Linear(hidden // 2, hidden // 4)\n        self.bn1234 = nn.BatchNorm1d(hidden // 4)\n        self.dropout1234 = nn.Dropout(dropout_rate)\n        \n        self.linear12345 = nn.Linear(hidden // 4, 2)\n        \n        # Task 2 (second branch)\n        self.linear2 = nn.Linear(hidden, 2*hidden)\n        self.bn2 = nn.BatchNorm1d(2*hidden)\n        self.dropout2 = nn.Dropout(dropout_rate)\n        \n        self.linear22 = nn.Linear(2*hidden, hidden)\n        self.bn22 = nn.BatchNorm1d(hidden)\n        self.dropout22 = nn.Dropout(dropout_rate)\n        \n        self.linear223 = nn.Linear(hidden, hidden // 2)\n        self.bn223 = nn.BatchNorm1d(hidden // 2)\n        self.dropout223 = nn.Dropout(dropout_rate)\n        \n        self.linear2234 = nn.Linear(hidden // 2, hidden // 4)\n        self.bn2234 = nn.BatchNorm1d(hidden // 4)\n        self.dropout2234 = nn.Dropout(dropout_rate)\n        \n        self.linear22345 = nn.Linear(hidden // 4, 2)\n\n    def forward(self, x, task):\n        if task == 0:\n            # Extract task-specific features for task 1\n            x = x[:,0,:]\n            x = self.linear1(x)\n            x = self.bn1(x)\n            x = torch.relu(x)\n            x = self.dropout1(x)\n            \n            x = self.linear12(x)\n            x = self.bn12(x)\n            x = torch.relu(x)\n            x = self.dropout12(x)\n            \n            x = self.linear123(x)\n            x = self.bn123(x)\n            x = torch.relu(x)\n            x = self.dropout123(x)\n            \n            x = self.linear1234(x)\n            x = self.bn1234(x)\n            x = torch.relu(x)\n            x = self.dropout1234(x)\n            \n            x = self.linear12345(x)\n            \n        else:\n            # Extract task-specific features for task 2\n            x=x[:,1,:]\n            x = self.linear2(x)\n            x = self.bn2(x)\n            x = torch.relu(x)\n            x = self.dropout2(x)\n            \n            x = self.linear22(x)\n            x = self.bn22(x)\n            x = torch.relu(x)\n            x = self.dropout22(x)\n            \n            x = self.linear223(x)\n            x = self.bn223(x)\n            x = torch.relu(x)\n            x = self.dropout223(x)\n            \n            x = self.linear2234(x)\n            x = self.bn2234(x)\n            x = torch.relu(x)\n            x = self.dropout2234(x)\n            \n            x = self.linear22345(x)\n        \n        return x\n\nclass BERTLM(torch.nn.Module):\n    def __init__(self, bert: BERT, vocab_size):\n        super().__init__()\n        self.bert = bert\n        self.class_lm=ClassificationModel(self.bert.embedding.embed_size)\n\n    def forward(self, x, attention_mask,task):\n        x = self.bert(x, attention_mask)\n        return self.class_lm(x,task)\n\nclass FeedForward(torch.nn.Module):\n    def __init__(self, d_model, middle_dim=2048, dropout=0.1):\n        super(FeedForward, self).__init__()\n        self.fc1 = torch.nn.Linear(d_model, middle_dim)\n        self.fc2 = torch.nn.Linear(middle_dim, d_model)\n        self.dropout = torch.nn.Dropout(dropout)\n        self.activation = torch.nn.GELU()\n\n    def forward(self, x):\n        out = self.activation(self.fc1(x))\n        out = self.fc2(self.dropout(out))\n        return out\n\nclass EncoderLayer(torch.nn.Module):\n    def __init__(self, d_model=EMBED_LEN, heads=8, feed_forward_hidden=EMBED_LEN*4, dropout=0.1):\n        super(EncoderLayer, self).__init__()\n        self.layernorm = torch.nn.LayerNorm(d_model)\n        self.self_multihead = MultiHeadedAttention(heads, d_model)\n        self.feed_forward = FeedForward(d_model, middle_dim=feed_forward_hidden)\n        self.dropout = torch.nn.Dropout(dropout)\n\n    def forward(self, embeddings, mask):\n        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n        interacted = self.layernorm(interacted + embeddings)\n        feed_forward_out = self.dropout(self.feed_forward(interacted))\n        encoded = self.layernorm(feed_forward_out + interacted)\n        return encoded\n\ndef differentiable_f1_loss(logits, labels, epsilon=1e-7):\n    \n    num_classes = logits.size(1)\n    labels_one_hot = F.one_hot(labels, num_classes).float()\n    probs = torch.softmax(logits, dim=1)\n    tp = (probs * labels_one_hot).sum(dim=0)\n    fp = ((1 - labels_one_hot) * probs).sum(dim=0)\n    fn = (labels_one_hot * (1 - probs)).sum(dim=0)\n    f1 = 2 * tp / (2 * tp + fp + fn + epsilon)\n    f1_loss = 1 - f1.mean()\n    return f1_loss\n\ndef train_bert(model, train_dataloader, optimizer,scheduler, device, epoch_start,epoch_end):\n    model.train()\n    for epoch in range(epoch_start,epoch_end):\n        loss_list=[]\n        total_loss__ = 0\n        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\"):\n            optimizer.zero_grad()\n            bert_input = batch['bert_input'].to(device)\n            bert_label = batch['bert_label'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            bert_label=bert_label.transpose(0,1)\n            for i in range(2):\n                outputs = model(bert_input, attention_mask,i)\n                \n                # print('outputs',outputs)\n                # print('labels',bert_label[i])\n                \n                # loss=F.cross_entropy(outputs,bert_label[i])\n                loss = differentiable_f1_loss(outputs,bert_label[i])\n                # print('loss',loss)\n                # if i==0:\n                #     reg_loss=model.class_lm.linear1.regularization_loss()\n                # else:\n                #     reg_loss=model.class_lm.linear2.regularization_loss()\n                # total_loss = loss + reg_loss\n                \n                # # Backpropagation\n                # total_loss.backward()\n                loss.backward()\n                # f1_loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                optimizer.step()\n                scheduler.step()\n                total_loss__ += loss.item()\n         \n        avg_loss = total_loss__ / (len(train_dataloader)*2)\n        print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss:.4f}\",'total loss',total_loss__)\n        # print(loss_list)\n\ndataset = BERTDataset(conv, tokenizer,labels)\ntrain_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n\nprint(f\"Vocabulary size: {tokenizer.get_vocab_size()}\")\nprint(f\"Number of sentences: {len(dataset)}\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\nbert = BERT(vocab_size=VOCAB_SIZE, d_model=EMBED_LEN, n_layers=6, heads=12)\nmodel = BERTLM(bert, vocab_size=VOCAB_SIZE)\nmodel.to(device)\n# optimizer = AdamW(model.parameters(), lr=2e-5)\n# optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\nWARMUP_STEPS=10000\nEPOCHS=20\ntotal_steps = len(train_loader) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=total_steps)","metadata":{"id":"XTthZzTranCU","trusted":true,"execution":{"iopub.status.busy":"2024-11-19T13:17:21.145471Z","iopub.execute_input":"2024-11-19T13:17:21.145818Z","iopub.status.idle":"2024-11-19T13:17:21.382967Z","shell.execute_reply.started":"2024-11-19T13:17:21.145787Z","shell.execute_reply":"2024-11-19T13:17:21.382033Z"}},"outputs":[{"name":"stdout","text":"Vocabulary size: 25000\nNumber of sentences: 6396\nUsing device: cuda\ntorch.Size([68, 288])\ndiv_term tensor([1.0000e+00, 9.3804e-01, 8.7992e-01, 8.2540e-01, 7.7426e-01, 7.2629e-01,\n        6.8129e-01, 6.3908e-01, 5.9948e-01, 5.6234e-01, 5.2750e-01, 4.9482e-01,\n        4.6416e-01, 4.3540e-01, 4.0842e-01, 3.8312e-01, 3.5938e-01, 3.3711e-01,\n        3.1623e-01, 2.9663e-01, 2.7826e-01, 2.6102e-01, 2.4484e-01, 2.2967e-01,\n        2.1544e-01, 2.0210e-01, 1.8957e-01, 1.7783e-01, 1.6681e-01, 1.5647e-01,\n        1.4678e-01, 1.3769e-01, 1.2915e-01, 1.2115e-01, 1.1365e-01, 1.0661e-01,\n        1.0000e-01, 9.3804e-02, 8.7992e-02, 8.2540e-02, 7.7426e-02, 7.2629e-02,\n        6.8129e-02, 6.3908e-02, 5.9948e-02, 5.6234e-02, 5.2750e-02, 4.9482e-02,\n        4.6416e-02, 4.3540e-02, 4.0842e-02, 3.8312e-02, 3.5938e-02, 3.3711e-02,\n        3.1623e-02, 2.9663e-02, 2.7826e-02, 2.6102e-02, 2.4484e-02, 2.2967e-02,\n        2.1544e-02, 2.0210e-02, 1.8957e-02, 1.7783e-02, 1.6681e-02, 1.5647e-02,\n        1.4678e-02, 1.3769e-02, 1.2916e-02, 1.2115e-02, 1.1365e-02, 1.0661e-02,\n        1.0000e-02, 9.3804e-03, 8.7992e-03, 8.2540e-03, 7.7426e-03, 7.2629e-03,\n        6.8129e-03, 6.3908e-03, 5.9948e-03, 5.6234e-03, 5.2750e-03, 4.9482e-03,\n        4.6416e-03, 4.3540e-03, 4.0842e-03, 3.8312e-03, 3.5938e-03, 3.3711e-03,\n        3.1623e-03, 2.9663e-03, 2.7826e-03, 2.6102e-03, 2.4484e-03, 2.2967e-03,\n        2.1544e-03, 2.0210e-03, 1.8957e-03, 1.7783e-03, 1.6681e-03, 1.5647e-03,\n        1.4678e-03, 1.3769e-03, 1.2915e-03, 1.2115e-03, 1.1365e-03, 1.0661e-03,\n        1.0000e-03, 9.3804e-04, 8.7992e-04, 8.2540e-04, 7.7426e-04, 7.2629e-04,\n        6.8129e-04, 6.3908e-04, 5.9948e-04, 5.6234e-04, 5.2750e-04, 4.9482e-04,\n        4.6416e-04, 4.3540e-04, 4.0842e-04, 3.8312e-04, 3.5938e-04, 3.3711e-04,\n        3.1623e-04, 2.9664e-04, 2.7826e-04, 2.6102e-04, 2.4484e-04, 2.2967e-04,\n        2.1544e-04, 2.0210e-04, 1.8957e-04, 1.7783e-04, 1.6681e-04, 1.5647e-04,\n        1.4678e-04, 1.3769e-04, 1.2915e-04, 1.2115e-04, 1.1365e-04, 1.0661e-04])\ndiv_term_dim torch.Size([144])\npe s torch.Size([68, 288])\npe tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n          0.0000e+00,  1.0000e+00],\n        [ 8.4147e-01,  5.4030e-01,  8.0640e-01,  ...,  1.0000e+00,\n          1.0661e-04,  1.0000e+00],\n        [ 9.0930e-01, -4.1615e-01,  9.5376e-01,  ...,  1.0000e+00,\n          2.1321e-04,  1.0000e+00],\n        ...,\n        [ 8.2683e-01, -5.6245e-01, -9.5872e-01,  ...,  9.9997e-01,\n          6.9293e-03,  9.9998e-01],\n        [-2.6551e-02, -9.9965e-01, -7.9626e-01,  ...,  9.9997e-01,\n          7.0359e-03,  9.9998e-01],\n        [-8.5552e-01, -5.1777e-01,  1.6951e-02,  ...,  9.9997e-01,\n          7.1425e-03,  9.9997e-01]])\n","output_type":"stream"}],"execution_count":137},{"cell_type":"code","source":"# tamil_text_files = ['/kaggle/input/wikidata/diamond_merged.txt','/kaggle/input/wikidata/main_train_valid.txt' ]\ntamil_text_files=['/kaggle/input/iconst/data.txt']\ntokenizer = create_tokenizer(tamil_text_files)  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T09:50:02.125245Z","iopub.execute_input":"2024-11-19T09:50:02.125583Z","iopub.status.idle":"2024-11-19T09:50:02.878493Z","shell.execute_reply.started":"2024-11-19T09:50:02.125555Z","shell.execute_reply":"2024-11-19T09:50:02.877608Z"}},"outputs":[{"name":"stdout","text":"\n\n\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"train_bert(model,train_loader,optimizer,scheduler,device,0,100) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T13:17:25.191230Z","iopub.execute_input":"2024-11-19T13:17:25.191847Z","iopub.status.idle":"2024-11-19T13:50:24.932717Z","shell.execute_reply.started":"2024-11-19T13:17:25.191813Z","shell.execute_reply":"2024-11-19T13:50:24.931586Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 3198/3198 [02:14<00:00, 23.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Average Loss: 0.5878 total loss 3759.7467392086983\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 3198/3198 [02:14<00:00, 23.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Average Loss: 0.5902 total loss 3775.1237441301346\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 3198/3198 [02:13<00:00, 23.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Average Loss: 0.5905 total loss 3776.8356679081917\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 3198/3198 [02:13<00:00, 23.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Average Loss: 0.5880 total loss 3761.049536347389\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 3198/3198 [02:14<00:00, 23.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Average Loss: 0.5842 total loss 3736.40359300375\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|██████████| 3198/3198 [02:14<00:00, 23.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6, Average Loss: 0.5826 total loss 3726.3539672493935\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|██████████| 3198/3198 [02:14<00:00, 23.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7, Average Loss: 0.5875 total loss 3757.781416654587\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8: 100%|██████████| 3198/3198 [02:15<00:00, 23.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8, Average Loss: 0.5849 total loss 3740.7491228580475\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9: 100%|██████████| 3198/3198 [02:14<00:00, 23.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9, Average Loss: 0.5837 total loss 3733.397730588913\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10: 100%|██████████| 3198/3198 [02:14<00:00, 23.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10, Average Loss: 0.5836 total loss 3732.991063416004\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11: 100%|██████████| 3198/3198 [02:14<00:00, 23.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11, Average Loss: 0.5834 total loss 3731.1475443840027\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12: 100%|██████████| 3198/3198 [02:14<00:00, 23.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12, Average Loss: 0.5840 total loss 3735.0812752246857\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13: 100%|██████████| 3198/3198 [02:14<00:00, 23.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13, Average Loss: 0.5835 total loss 3732.1876333355904\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14: 100%|██████████| 3198/3198 [02:14<00:00, 23.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14, Average Loss: 0.5837 total loss 3733.0494379401207\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15:  73%|███████▎  | 2347/3198 [01:39<00:35, 23.69it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[138], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_bert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m \n","Cell \u001b[0;32mIn[137], line 361\u001b[0m, in \u001b[0;36mtrain_bert\u001b[0;34m(model, train_dataloader, optimizer, scheduler, device, epoch_start, epoch_end)\u001b[0m\n\u001b[1;32m    351\u001b[0m loss \u001b[38;5;241m=\u001b[39m differentiable_f1_loss(outputs,bert_label[i])\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# print('loss',loss)\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# if i==0:\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m#     reg_loss=model.class_lm.linear1.regularization_loss()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;66;03m# # Backpropagation\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;66;03m# total_loss.backward()\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# f1_loss.backward()\u001b[39;00m\n\u001b[1;32m    363\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":138},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T10:57:21.469921Z","iopub.execute_input":"2024-11-19T10:57:21.470696Z","iopub.status.idle":"2024-11-19T10:57:21.477132Z","shell.execute_reply.started":"2024-11-19T10:57:21.470664Z","shell.execute_reply":"2024-11-19T10:57:21.476249Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"BERTLM(\n  (bert): BERT(\n    (embedding): BERTEmbedding(\n      (token): Embedding(30000, 288, padding_idx=0)\n      (position): PositionalEmbedding()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder_layers): ModuleList(\n      (0-11): 12 x EncoderLayer(\n        (layernorm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n        (self_multihead): MultiHeadedAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (query): Linear(in_features=288, out_features=288, bias=True)\n          (key): Linear(in_features=288, out_features=288, bias=True)\n          (value): Linear(in_features=288, out_features=288, bias=True)\n          (output_linear): Linear(in_features=288, out_features=288, bias=True)\n        )\n        (feed_forward): FeedForward(\n          (fc1): Linear(in_features=288, out_features=1152, bias=True)\n          (fc2): Linear(in_features=1152, out_features=288, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): GELU(approximate='none')\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (class_lm): ClassificationModel(\n    (linear1): Linear(in_features=288, out_features=576, bias=True)\n    (bn1): BatchNorm1d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (dropout1): Dropout(p=0.5, inplace=False)\n    (linear12): Linear(in_features=576, out_features=288, bias=True)\n    (bn12): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (dropout12): Dropout(p=0.5, inplace=False)\n    (linear123): Linear(in_features=288, out_features=144, bias=True)\n    (bn123): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (dropout123): Dropout(p=0.5, inplace=False)\n    (linear1234): Linear(in_features=144, out_features=72, bias=True)\n    (bn1234): BatchNorm1d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (dropout1234): Dropout(p=0.5, inplace=False)\n    (linear12345): Linear(in_features=72, out_features=2, bias=True)\n    (linear2): Linear(in_features=288, out_features=576, bias=True)\n    (bn2): BatchNorm1d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (dropout2): Dropout(p=0.5, inplace=False)\n    (linear22): Linear(in_features=576, out_features=288, bias=True)\n    (bn22): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (dropout22): Dropout(p=0.5, inplace=False)\n    (linear223): Linear(in_features=288, out_features=144, bias=True)\n    (bn223): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (dropout223): Dropout(p=0.5, inplace=False)\n    (linear2234): Linear(in_features=144, out_features=72, bias=True)\n    (bn2234): BatchNorm1d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (dropout2234): Dropout(p=0.5, inplace=False)\n    (linear22345): Linear(in_features=72, out_features=2, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"dataset.__getitem__(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T10:59:01.943855Z","iopub.execute_input":"2024-11-19T10:59:01.944573Z","iopub.status.idle":"2024-11-19T10:59:01.952121Z","shell.execute_reply.started":"2024-11-19T10:59:01.944537Z","shell.execute_reply":"2024-11-19T10:59:01.951273Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"{'bert_input': tensor([ 137, 1047,  404,  371,  154,  449,  153,  938,  486,  545,  416,  700,\n          955,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n            2,    2,    2,    2,    2,    2,    2,    2]),\n 'bert_label': tensor([0, 0, 0, 0]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"list_features={}\nlist_F=[]\nfor i in range(6396):\n    tokens = dataset.__getitem__(i)['bert_input'].unsqueeze(1)\n    tokens=tokens.to(device)\n    abc=model.bert.embedding(tokens)\n    list_F.append(abc.reshape(68*288).tolist())\n    list_features[f'{i}']=abc.reshape(68*288).tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T12:27:56.942180Z","iopub.execute_input":"2024-11-19T12:27:56.943028Z","iopub.status.idle":"2024-11-19T12:28:09.176263Z","shell.execute_reply.started":"2024-11-19T12:27:56.942982Z","shell.execute_reply":"2024-11-19T12:28:09.175558Z"}},"outputs":[],"execution_count":81},{"cell_type":"code","source":"label_list=[]\nfor i in range(6396):\n    label_list.append(int(labels[i][0]))\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T12:56:54.419119Z","iopub.execute_input":"2024-11-19T12:56:54.419472Z","iopub.status.idle":"2024-11-19T12:56:54.425646Z","shell.execute_reply.started":"2024-11-19T12:56:54.419441Z","shell.execute_reply":"2024-11-19T12:56:54.424775Z"}},"outputs":[],"execution_count":111},{"cell_type":"raw","source":"len(label_list)","metadata":{"execution":{"iopub.status.busy":"2024-11-18T11:56:46.424110Z","iopub.execute_input":"2024-11-18T11:56:46.424800Z","iopub.status.idle":"2024-11-18T11:56:46.465673Z","shell.execute_reply.started":"2024-11-18T11:56:46.424763Z","shell.execute_reply":"2024-11-18T11:56:46.464532Z"}}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test=train_test_split(list_F,label_list)\nlen(x_train),len(x_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T12:57:04.208665Z","iopub.execute_input":"2024-11-19T12:57:04.209568Z","iopub.status.idle":"2024-11-19T12:57:04.218067Z","shell.execute_reply.started":"2024-11-19T12:57:04.209531Z","shell.execute_reply":"2024-11-19T12:57:04.217164Z"}},"outputs":[{"execution_count":112,"output_type":"execute_result","data":{"text/plain":"(4797, 1599)"},"metadata":{}}],"execution_count":112},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import f1_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T12:41:11.360958Z","iopub.execute_input":"2024-11-19T12:41:11.361349Z","iopub.status.idle":"2024-11-19T12:41:11.365733Z","shell.execute_reply.started":"2024-11-19T12:41:11.361319Z","shell.execute_reply":"2024-11-19T12:41:11.364771Z"}},"outputs":[],"execution_count":96},{"cell_type":"code","source":"dmodel=DecisionTreeClassifier()\ndmodel.fit(x_train,y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T12:57:27.188727Z","iopub.execute_input":"2024-11-19T12:57:27.189586Z","iopub.status.idle":"2024-11-19T12:58:53.773953Z","shell.execute_reply.started":"2024-11-19T12:57:27.189549Z","shell.execute_reply":"2024-11-19T12:58:53.773161Z"}},"outputs":[{"execution_count":115,"output_type":"execute_result","data":{"text/plain":"DecisionTreeClassifier()","text/html":"<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"},"metadata":{}}],"execution_count":115},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder(sparse_output=False)\none_hot_encoded = encoder.fit_transform(np.array(y_test).reshape(-1,1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T13:00:03.659842Z","iopub.execute_input":"2024-11-19T13:00:03.660196Z","iopub.status.idle":"2024-11-19T13:00:03.666117Z","shell.execute_reply.started":"2024-11-19T13:00:03.660165Z","shell.execute_reply":"2024-11-19T13:00:03.665323Z"}},"outputs":[],"execution_count":119},{"cell_type":"code","source":"pred=dmodel.predict(x_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T12:59:41.498563Z","iopub.execute_input":"2024-11-19T12:59:41.498903Z","iopub.status.idle":"2024-11-19T12:59:42.794942Z","shell.execute_reply.started":"2024-11-19T12:59:41.498873Z","shell.execute_reply":"2024-11-19T12:59:42.794045Z"}},"outputs":[],"execution_count":117},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"one_hot_encoded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T13:00:08.148386Z","iopub.execute_input":"2024-11-19T13:00:08.149050Z","iopub.status.idle":"2024-11-19T13:00:08.154855Z","shell.execute_reply.started":"2024-11-19T13:00:08.148999Z","shell.execute_reply":"2024-11-19T13:00:08.153810Z"}},"outputs":[{"execution_count":120,"output_type":"execute_result","data":{"text/plain":"array([[0., 1.],\n       [0., 1.],\n       [0., 1.],\n       ...,\n       [1., 0.],\n       [0., 1.],\n       [1., 0.]])"},"metadata":{}}],"execution_count":120},{"cell_type":"code","source":"from sklearn.metrics import f1_score\nimport numpy as np\n\nbinary_labels = np.argmax(one_hot_encoded, axis=1)  # Convert one-hot to binary labels\nscore = f1_score(binary_labels, pred, average='macro')  # Use an appropriate average\nprint(score)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T13:00:11.726815Z","iopub.execute_input":"2024-11-19T13:00:11.727416Z","iopub.status.idle":"2024-11-19T13:00:11.734731Z","shell.execute_reply.started":"2024-11-19T13:00:11.727382Z","shell.execute_reply":"2024-11-19T13:00:11.733800Z"}},"outputs":[{"name":"stdout","text":"0.517469357161441\n","output_type":"stream"}],"execution_count":121},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"raw","source":"","metadata":{"execution":{"iopub.status.busy":"2024-11-18T11:56:46.424110Z","iopub.execute_input":"2024-11-18T11:56:46.424800Z","iopub.status.idle":"2024-11-18T11:56:46.465673Z","shell.execute_reply.started":"2024-11-18T11:56:46.424763Z","shell.execute_reply":"2024-11-18T11:56:46.464532Z"}}},{"cell_type":"markdown","source":"def load_model(load_directory,model):\n    model_path = os.path.join(load_directory, \"bert_model_17_epoch.pth\")\n    print(f\"Model and vocabulary loaded from {load_directory}\")\n    model.load_state_dict(torch.load(model_path))\n    return model\nmodel=load_model(\"/kaggle/input/bert/pytorch/abc/1\", model)","metadata":{"execution":{"iopub.status.busy":"2024-09-24T16:34:22.902706Z","iopub.execute_input":"2024-09-24T16:34:22.903237Z","iopub.status.idle":"2024-09-24T16:34:23.941653Z","shell.execute_reply.started":"2024-09-24T16:34:22.903193Z","shell.execute_reply":"2024-09-24T16:34:23.940579Z"}}},{"cell_type":"code","source":"# def load_model(load_directory,model): \n#     model_path = os.path.join(load_directory, \"bert_model_24_epoch.pth\") \n#     print(f\"Model and vocabulary loaded from {load_directory}\") \n#     model.load_state_dict(torch.load(model_path)) \n#     return model \n# model=load_model(\"/kaggle/input/abcd/pytorch/default/1/\", model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\noutput_file = 'data.json'\nwith open(output_file, 'w') as f:\n    json.dump(list_features, f, indent=4) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T12:12:28.185515Z","iopub.execute_input":"2024-11-19T12:12:28.186215Z","iopub.status.idle":"2024-11-19T12:15:15.871530Z","shell.execute_reply.started":"2024-11-19T12:12:28.186180Z","shell.execute_reply":"2024-11-19T12:15:15.870789Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"model.bert","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.bert.embedding.position.pe.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_embedding=model.bert.embedding.token.weight\nvocab = tokenizer.get_vocab()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"extract embedding from bert","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\ndef get_token_embeddings_custom_bert(model, input_ids):\n    print(input_ids)\n    with torch.no_grad():\n        embeddings = model.bert.embedding(input_ids) \n        for layer in model.bert.encoder_layers:\n            embeddings = layer(embeddings)  \n    \n    return embeddings\n\ndef get_embedding(model, tokenizer, word, top_n=10):\n    dataset = BERTDataset([word], tokenizer)\n    data=dataset.__getitem__(0,flag=False)\n    input_ids = data['bert_input'].unsqueeze(0).to(device)\n    attention_mask = data['attention_mask'].unsqueeze(0).to(device)\n    masked_lm_labels = data['bert_label'].unsqueeze(0).to(device)  \n    \n    word_embeddings = get_token_embeddings_custom_bert(model, input_ids)\n    word_embedding = torch.mean(word_embeddings, dim=1).squeeze(0)\n\n    # Now compute cosine similarity between this word's embedding and all token embeddings\n    all_embeddings = model.bert.embedding.token.weight \n    cosine_similarities = F.cosine_similarity(word_embedding.unsqueeze(0), all_embeddings, dim=1)\n\n    # Get the top N most similar tokens (excluding the queried word itself)\n    top_indices = torch.argsort(cosine_similarities, descending=True)[1:top_n+1]\n    \n    # Convert the token indices back to words\n    similar_words = [tokenizer.convert_ids_to_tokens(idx.item()) for idx in top_indices]\n    \n    print(f\"Top {top_n} words similar to '{word}': {similar_words}\")\n\nword = \"குடியரசு\"\nget_embedding(model, tokenizer, word, top_n=10)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"find_similar_words('முதல',tokenizer,model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"special_tokens = [\"[PAD]\", \"[MASK]\", \"[UNK]\"]\nvocab=tokenizer.get_vocab()\nlen(vocab)\nfor i in special_tokens:\n    print(vocab[i])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"key_int_value_word={}\nfor key,value in vocab.items():\n    key_int_value_word[value]=key","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"check the unknown count in the built tensors for the sentence input","metadata":{}},{"cell_type":"code","source":"# count=0\n# for i in range(80000,85000):\n#     example = dataset[i]\n# #     print(f\"Example {i + 1}:\")\n#     a= example['bert_input']\n# #     print(\"BERT Label Tokens: \", example['bert_label'])\n# #     print(\"Attention Mask: \", example['attention_mask'])\n# # #     print() \n#     a=a.tolist()\n# #     l=[]\n#     for j in a:\n# #         l.append(key_int_value_word[j])\n#         if j==2:\n#             count+=1\n#         if j==0:\n#             break\n#     if i%1000==0:\n#             print(i)\n# #         print(j,end=' ')\n# #     print()\n# #     print(l)\n# print(count)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vocab=tokenizer.get_vocab()\n# sorted_vocab = dict(sorted(vocab.items(), key=lambda item: item[1]))\n\n# with open('output_wiki_test1.txt','w',encoding='utf-8') as f:\n#     for key,value in sorted_vocab.items():\n#         f.write(f'{key}\\t\\t{value} \\n')   ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef test_bert_model(model, tokenizer, sentence, device,target,top_n=5):\n    dataset = BERTDataset([sentence], tokenizer)\n    dataset_target=BERTDataset([target],tokenizer)\n    data_target=dataset_target.__getitem__(0,False)\n    data=dataset.__getitem__(0,False)\n#     print(data)\n#     print(data_target)\n    target_list_tokenized=[]\n    target_input_ids=data_target['bert_input'].unsqueeze(0).to(device)\n    target_list=target_input_ids.tolist()\n    \n    for i in target_list[0]:\n        if i==0:\n            break\n        else:\n            target_list_tokenized.append(i)\n    #print('target',target_list_tokenized)\n    input_ids = data['bert_input'].unsqueeze(0).to(device)\n    #print(input_ids)\n    attention_mask = data['attention_mask'].unsqueeze(0).to(device)\n    \n    key_indexes=[input_ids.tolist()[0].index(i) for i in target_list_tokenized ]\n    print(key_indexes)\n    \n    #print(input_ids.shape)\n    model.eval()\n    predictions=[]\n    with torch.no_grad():\n        outputs=model.bert(input_ids, attention_mask)\n        #print('out',outputs.shape)\n    \n    context_embedding_list=[]\n    output=outputs.tolist()\n    for i in key_indexes:\n        context_embedding_list.append(output[0][i])\n    context_embedding=[0]*252\n    for context in context_embedding_list:\n        context_embedding = [context_embedding[i] + context[i] for i in range(len(context))]\n   # print(len(context_embedding))\n    context_embedding_tensor = torch.tensor(context_embedding, dtype=torch.float32).to(device)\n    #print(context_embedding_tensor)\n    all_embeddings = model.bert.embedding.token.weight \n    cosine_similarities = F.cosine_similarity(context_embedding_tensor.unsqueeze(0), all_embeddings, dim=1)\n\n    # Get the top N most similar tokens (excluding the queried word itself)\n    top_indices = torch.argsort(cosine_similarities, descending=True)[1:10]\n    print('top',top_indices)\n    # Convert the token indices back to words\n    return [tokenizer.id_to_token(idx.item()) for idx in top_indices]\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_sentence = \"அகர முதல எழுத்தெல்லாம் ஆதி பகவன் முதற்றே உலகு\"\nprint(test_bert_model(model, tokenizer, test_sentence, device,\"அகர\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\ndef predict_masked_words(model, tokenizer, sentence, device):\n    model.eval()\n    tokens = tokenizer.encode(sentence).ids\n    print('tokens',tokens)\n    mask_token_id = tokenizer.token_to_id('[MASK]')\n    print('mask token id',mask_token_id)\n    mask_positions = [i for i, token in enumerate(tokens) if token == mask_token_id]\n    print('mask_position',mask_positions)\n    input_ids = torch.tensor([tokens]).to(device)\n    print('input ids',input_ids)\n    attention_mask = torch.ones_like(input_ids).to(device)\n    print('att',attention_mask)\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask)\n    print('outputs',outputs.shape)\n    predictions = []\n    for pos in range(10):\n        predicted_id = outputs[0, pos].argmax().item()\n        predicted_token = tokenizer.id_to_token(predicted_id)\n        predictions.append((pos, predicted_token))\n    \n    return predictions\n\n# def analyze_training_performance(losses, epochs):\n#     import matplotlib.pyplot as plt\n    \n#     plt.figure(figsize=(10, 6))\n#     plt.plot(range(1, epochs+1), losses)\n#     plt.title('Training Loss over Epochs')\n#     plt.xlabel('Epoch')\n#     plt.ylabel('Loss')\n#     plt.grid(True)\n#     plt.show()\n    \n#     print(f\"Initial loss: {losses[0]:.4f}\")\n#     print(f\"Final loss: {losses[-1]:.4f}\")\n#     print(f\"Absolute improvement: {losses[0] - losses[-1]:.4f}\")\n#     print(f\"Relative improvement: {(losses[0] - losses[-1]) / losses[0] * 100:.2f}%\")\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\ntest_sentence = \"அகர [MASK] எழுத்தெல்லாம் ஆதி [MASK] முதற்றே உலகு\"\npredictions = predict_masked_words(model, tokenizer, test_sentence, device)\n\nprint(\"Original sentence:\", test_sentence)\nprint(\"Predictions:\")\nfor pos, pred in predictions:\n    print(f\"Position {pos}: {pred}\")\n\n# # Assuming you've collected losses during training\n# training_losses = [...]  # List of loss values from each epoch\n# analyze_training_performance(training_losses, len(training_losses))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# torch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# save_directory = \"../models/diamond_merged_tokenized\"\n# save_model(model, tokenizer, save_directory)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef load_model(load_directory,model):\n    model_path = os.path.join(load_directory, \"bert_model11th_epoch.pth\")\n#     bert = BERT(vocab_size=vocab_size, d_model=d_model, n_layers=n_layers, heads=heads)\n#     model = BERTLM(bert, vocab_size=vocab_size)\n    print(f\"Model and vocabulary loaded from {load_directory}\")\n    model.load_state_dict(torch.load(model_path))\n    return model\n#     vocab_path = os.path.join(load_directory, \"vocab.json\")\n#     with open(vocab_path, 'r', encoding='utf-8') as f:\n#         vocab = json.load(f)\n    \n#     tokenizer = Tokenizer(models.WordLevel(vocab=vocab, unk_token=\"[UNK]\"))\n#     tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n    \n#     special_tokens = [\"[PAD]\",\"[MASK]\", \"[UNK]\"]\n#     tokenizer.add_special_tokens(special_tokens)\n\n#     tokenizer.post_processor = processors.TemplateProcessing(\n#         single=\"[CLS] $A [SEP]\",\n#         pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n#         special_tokens=[\n#             (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n#             (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n#         ],\n#     )\n    \n   \n#     return model, tokenizer\nmodel=load_model(\"/kaggle/input/bert/pytorch/default/1\", model)\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# loaded_model.to(device)\n\n# sentence = \"கற்றதனால் ஆய எல்லா பயன் என்கொல் நூல்களையும் கற்றவர்க்கு அக்கல்வி அறிவினாலாய பயன் யாது\"\n# word = \"என்கொல்\"\n# similar_words = find_contextual_similar_words(sentence, word, tokenizer, model, device)\n# print(f\"Words similar to '{word}' in the context of '{sentence}': {similar_words}\")\n\n# masked_sentence = \"கற்றதனால் [MASK] பயன் என்கொல் \"\n# predicted_word, predicted_sentence = predict_masked_word(masked_sentence, tokenizer, model, device)\n# print(f\"Original masked sentence: {masked_sentence}\")\n# print(f\"Predicted word: {predicted_word}\")\n# print(f\"Predicted sentence: {predicted_sentence}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef find_contextual_similar_words(sentence, word, tokenizer, model, device, top_n=5):\n    model.eval()\n    with torch.no_grad():\n        tokens = [tokenizer.token_to_id(token) for token in sentence.split()]\n        word_id = tokenizer.token_to_id(word)\n        if word_id is None or word_id not in tokens:\n            return []\n        word_position = tokens.index(word_id)\n        print(tokens)\n        input_ids = torch.tensor([tokens]).to(device)\n        attention_mask = torch.ones_like(input_ids).to(device)\n        outputs = model.bert(input_ids, attention_mask)\n        contextual_embedding = outputs[0, word_position]\n        all_embeddings = model.bert.embedding.token.weight\n        cosine_similarities = F.cosine_similarity(contextual_embedding, all_embeddings)\n        top_indices = torch.argsort(cosine_similarities, descending=True)[1:top_n+1]\n        similar_words = [tokenizer.id_to_token(idx.item()) for idx in top_indices]\n        \n        return similar_words\n\nsentence = \"வானூர்தி நிலையங்கள் உள்ளன\"\nword = \"வானூர்தி\"\nsimilar_words = find_contextual_similar_words(sentence, word, tokenizer, model, device)\nprint(f\"Words similar to '{word}' in the context of '{sentence}': {similar_words}\") ","metadata":{"id":"tvuR898Z8ZkK","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"***claude code***","metadata":{"id":"fr4T99aD8XwC"}},{"cell_type":"code","source":"","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"duDDjVH2_3Dp","outputId":"4917c691-489b-4b7b-e2b1-837f5e68f114","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with torch.no_grad():\n        word_id = tokenizer.token_to_id(\"விரிநீர்\")\n        if word_id is None:\n            print('None')\n        word_embedding = model.bert.embedding.token.weight[word_id]\n        cosine_similarities = F.cosine_similarity(word_embedding, model.bert.embedding.token.weight)\n        top_indices = torch.argsort(cosine_similarities, descending=True)[1:5]\n        print([tokenizer.id_to_token(idx.item()) for idx in top_indices])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xZnSzTyREp2X","outputId":"c3b59dba-ef85-4f7f-f79e-73a155ca3778","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def print_vocabulary(tokenizer):\n#     vocab = tokenizer.get_vocab()\n#     sorted_vocab=sorted(vocab.items(), key=lambda x: x[1])\n#     print(\"Vocabulary:\")\n#     for word, index in sorted_vocab:\n#         print(word,end=' ')\n#     print(f\"Total vocabulary size: {len(vocab)}\")\n","metadata":{"id":"ZWbvmAh2E6H_","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}